{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96742406",
   "metadata": {
    "id": "b5cFq_TgWlQ_",
    "papermill": {
     "duration": 0.011926,
     "end_time": "2023-05-25T14:58:40.204560",
     "exception": false,
     "start_time": "2023-05-25T14:58:40.192634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Homework 11 - Transfer Learning (Domain Adversarial Training)\n",
    "\n",
    "> Author: Howard Wang (b08902047@ntu.edu.tw)\n",
    "\n",
    "If there are any questions, please contact mlta-2023-spring@googlegroups.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7d91282",
   "metadata": {
    "id": "vNiZCGrIYKdR",
    "papermill": {
     "duration": 0.010998,
     "end_time": "2023-05-25T14:58:40.227024",
     "exception": false,
     "start_time": "2023-05-25T14:58:40.216026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Readme\n",
    "\n",
    "In homework 11, you will need to implement Domain Adversarial Training in Transfer Learning. As shown in the bottom left part of the figure.\n",
    "\n",
    "<img src=\"https://i.imgur.com/iMVIxCH.png\" width=\"500px\">\n",
    "\n",
    "> \n",
    "\n",
    "## Scenario and Why Domain Adversarial Training\n",
    "Now we have labeled source data and unlabeled target data, where source data might be relavent to the target data. We now want to train a model with source data only and test it on target data.\n",
    "\n",
    "What problem might occur if we do so? After we have learned Anomaly Detection, we now know that if we test the model with an abnormal data that have never appeared in source data, our trained model is likely to result in poor performance since it is not familiar with the abnormal data.\n",
    "\n",
    "For example, we have a model that contains Feature Extractor and Classifier:\n",
    "<img src=\"https://i.imgur.com/IL0PxCY.png\" width=\"500px\">\n",
    "\n",
    "When the model is trained with source data, the feature extractor \n",
    "will extract meaningful features since it is familiar with the distribution of it.It could be seen in the following figure that the blue dots, which is the distribution of source data, has already been clustered into different clusters. Therefore, the Classifier can predict the label based on these clusters.\n",
    "\n",
    "However, when test on the target data, the Feature Extractor will not be able to extract meaningful features that follow the distribution of the source feature distribution, which result in the classifier learned for the source domain will not be able to apply to the target domain.\n",
    "\n",
    "\n",
    "## Domain Adversarial Training of Nerural Networks (DaNN)\n",
    "\n",
    "Based on the above problems, DaNN approaches build mappings between the source (training-time) and the target (test-time) domains, so that the classifier learned for the source domain can also be applied to the target domain, when composed with the learned mapping between domains.\n",
    "\n",
    "<img src=\"https://i.imgur.com/vrOE5a6.png\" width=\"500px\">\n",
    "\n",
    "In DaNN, the authors added a Domain Classifier, which is a deep discriminatively-trained classifeir in the training framework to distinguish the data from different domain by the features extracted by the feature extractor. As the training progresses, the approach promotes a domain classifier that discriminates between the source and the target domains and a feature extractor that can extractor features that are discriminative for the main learning task on the source domain and indiscriminate with respect to the shift between the domains. \n",
    "\n",
    "\n",
    "The feature extractor are likely to outperform the domain classifier as its input are generated by the feature extractor and that the task of domain classification and label classification are not conflict.\n",
    "\n",
    "This method leads to the emergence of features that are domain-invariant and on the same feature distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d65af241",
   "metadata": {
    "id": "3-qnUkspmap3",
    "papermill": {
     "duration": 0.010984,
     "end_time": "2023-05-25T14:58:40.249176",
     "exception": false,
     "start_time": "2023-05-25T14:58:40.238192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Introduce\n",
    "\n",
    "Our task contains source data: real photos, and target data: hand-drawn graffiti.\n",
    "\n",
    "We are going to train the model with the photos and the labels, and try to predict what the labels are for hand-drawn graffiti.\n",
    "\n",
    "The data could be downloaded [here](https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.0/real_or_drawing.zip). The code below is for data downloading and visualization.\n",
    "\n",
    "Note that: **The source and target data are all balanced data, you can make use of this information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874fd822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T14:58:40.273551Z",
     "iopub.status.busy": "2023-05-25T14:58:40.272844Z",
     "iopub.status.idle": "2023-05-25T14:58:52.800713Z",
     "shell.execute_reply": "2023-05-25T14:58:52.799459Z"
    },
    "id": "DF-i0sVlnUbq",
    "outputId": "db9c3dbb-22d6-4c72-d6d5-7fa18322a51e",
    "papermill": {
     "duration": 12.542877,
     "end_time": "2023-05-25T14:58:52.803247",
     "exception": false,
     "start_time": "2023-05-25T14:58:40.260370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-25 14:58:41--  https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.0/real_or_drawing.zip\r\n",
      "Resolving github.com (github.com)... 192.30.255.113\r\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/465172560/0c631a4e-fa19-4c31-983d-c25ba7776d7a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230525%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230525T145841Z&X-Amz-Expires=300&X-Amz-Signature=f94c2cc3acea2455dbb37a642009bbabd24d1293b4f6300efed84d8fbc5f75c9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=465172560&response-content-disposition=attachment%3B%20filename%3Dreal_or_drawing.zip&response-content-type=application%2Foctet-stream [following]\r\n",
      "--2023-05-25 14:58:41--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/465172560/0c631a4e-fa19-4c31-983d-c25ba7776d7a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230525%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230525T145841Z&X-Amz-Expires=300&X-Amz-Signature=f94c2cc3acea2455dbb37a642009bbabd24d1293b4f6300efed84d8fbc5f75c9&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=465172560&response-content-disposition=attachment%3B%20filename%3Dreal_or_drawing.zip&response-content-type=application%2Foctet-stream\r\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\r\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 107293358 (102M) [application/octet-stream]\r\n",
      "Saving to: ‘real_or_drawing.zip’\r\n",
      "\r\n",
      "real_or_drawing.zip 100%[===================>] 102.32M  75.1MB/s    in 1.4s    \r\n",
      "\r\n",
      "2023-05-25 14:58:43 (75.1 MB/s) - ‘real_or_drawing.zip’ saved [107293358/107293358]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#Download dataset\n",
    "!wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.0/real_or_drawing.zip\" -O real_or_drawing.zip\n",
    "\n",
    "# Download from mirrored dataset link\n",
    "# !wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.1/real_or_drawing.zip\" -O real_or_drawing.zip\n",
    "# !wget \"https://github.com/redxouls/ml2020spring-hw11-dataset/releases/download/v1.0.2/real_or_drawing.zip\" -O real_or_drawing.zip\n",
    "# Unzip the files\n",
    "!unzip -q real_or_drawing.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c77883a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T14:58:52.830069Z",
     "iopub.status.busy": "2023-05-25T14:58:52.829464Z",
     "iopub.status.idle": "2023-05-25T14:58:57.137567Z",
     "shell.execute_reply": "2023-05-25T14:58:57.136571Z"
    },
    "id": "0_uO-ZSDoR6i",
    "outputId": "72879497-ab04-4eac-9a61-aae79a399c96",
    "papermill": {
     "duration": 4.326179,
     "end_time": "2023-05-25T14:58:57.142046",
     "exception": false,
     "start_time": "2023-05-25T14:58:52.815867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def no_axis_show(img, title='', cmap=None):\n",
    "  # imshow, and set the interpolation mode to be \"nearest\"。\n",
    "  fig = plt.imshow(img, interpolation='nearest', cmap=cmap)\n",
    "  # do not show the axes in the images.\n",
    "  fig.axes.get_xaxis().set_visible(False)\n",
    "  fig.axes.get_yaxis().set_visible(False)\n",
    "  plt.title(title)\n",
    "\n",
    "\n",
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show((plt.imread(f'real_or_drawing/train_data/{i}/{500*i}.bmp')), title=titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345fb563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T14:58:57.180561Z",
     "iopub.status.busy": "2023-05-25T14:58:57.180026Z",
     "iopub.status.idle": "2023-05-25T14:58:57.524782Z",
     "shell.execute_reply": "2023-05-25T14:58:57.523512Z"
    },
    "id": "3eMs7DbVt4Ee",
    "outputId": "488467f8-66d1-4ee1-814e-46c9d9dc4ca3",
    "papermill": {
     "duration": 0.361329,
     "end_time": "2023-05-25T14:58:57.527165",
     "exception": false,
     "start_time": "2023-05-25T14:58:57.165836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(10):\n",
    "  plt.subplot(1, 10, i+1)\n",
    "  fig = no_axis_show(plt.imread(f'real_or_drawing/test_data/0/' + str(i).rjust(5, '0') + '.bmp'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e52b835",
   "metadata": {
    "id": "moXQw9To5TqZ",
    "papermill": {
     "duration": 0.024306,
     "end_time": "2023-05-25T14:58:57.580920",
     "exception": false,
     "start_time": "2023-05-25T14:58:57.556614",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Special Domain Knowledge\n",
    "\n",
    "When we graffiti, we usually draw the outline only, therefore we can perform edge detection processing on the source data to make it more similar to the target data.\n",
    "\n",
    "\n",
    "## Canny Edge Detection\n",
    "The implementation of Canny Edge Detection is as follow.\n",
    "The algorithm will not be describe thoroughly here.  If you are interested, please refer to the wiki or [here](https://medium.com/@pomelyu5199/canny-edge-detector-%E5%AF%A6%E4%BD%9C-opencv-f7d1a0a57d19).\n",
    "\n",
    "We only need two parameters to implement Canny Edge Detection with CV2:  `low_threshold` and `high_threshold`.\n",
    "\n",
    "```cv2.Canny(image, low_threshold, high_threshold)```\n",
    "\n",
    "Simply put, when the edge value exceeds the high_threshold, we determine it as an edge. If the edge value is only above low_threshold, we will then determine whether it is an edge or not.\n",
    "\n",
    "Let's implement it on the source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7032fe34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T14:58:57.608535Z",
     "iopub.status.busy": "2023-05-25T14:58:57.608206Z",
     "iopub.status.idle": "2023-05-25T14:58:58.284469Z",
     "shell.execute_reply": "2023-05-25T14:58:58.283449Z"
    },
    "id": "mn2MkDLV7E2-",
    "outputId": "b9294566-ffa8-42da-82a6-0ac7a1c5638b",
    "papermill": {
     "duration": 0.693596,
     "end_time": "2023-05-25T14:58:58.287632",
     "exception": false,
     "start_time": "2023-05-25T14:58:57.594036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1800 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "titles = ['horse', 'bed', 'clock', 'apple', 'cat', 'plane', 'television', 'dog', 'dolphin', 'spider']\n",
    "plt.figure(figsize=(18, 18))\n",
    "\n",
    "original_img = plt.imread(f'real_or_drawing/train_data/0/0.bmp')\n",
    "plt.subplot(1, 5, 1)\n",
    "no_axis_show(original_img, title='original')\n",
    "\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "plt.subplot(1, 5, 2)\n",
    "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
    "\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "plt.subplot(1, 5, 2)\n",
    "no_axis_show(gray_img, title='gray scale', cmap='gray')\n",
    "\n",
    "canny_50100 = cv2.Canny(gray_img, 50, 100)\n",
    "plt.subplot(1, 5, 3)\n",
    "no_axis_show(canny_50100, title='Canny(50, 100)', cmap='gray')\n",
    "\n",
    "canny_150200 = cv2.Canny(gray_img, 150, 200)\n",
    "plt.subplot(1, 5, 4)\n",
    "no_axis_show(canny_150200, title='Canny(150, 200)', cmap='gray')\n",
    "\n",
    "canny_250300 = cv2.Canny(gray_img, 250, 300)\n",
    "plt.subplot(1, 5, 5)\n",
    "no_axis_show(canny_250300, title='Canny(250, 300)', cmap='gray')\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0215f19",
   "metadata": {
    "id": "8THSdt_hmwYh",
    "papermill": {
     "duration": 0.013725,
     "end_time": "2023-05-25T14:58:58.323879",
     "exception": false,
     "start_time": "2023-05-25T14:58:58.310154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Process\n",
    " \n",
    " \n",
    "The data is suitible for `torchvision.ImageFolder`. You can create a dataset with `torchvision.ImageFolder`. Details for image augmentation please refer to the comments in the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533dd97a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T14:58:58.353009Z",
     "iopub.status.busy": "2023-05-25T14:58:58.352703Z",
     "iopub.status.idle": "2023-05-25T14:58:58.983297Z",
     "shell.execute_reply": "2023-05-25T14:58:58.982328Z"
    },
    "id": "WZHIBGknmi8Z",
    "papermill": {
     "duration": 0.647974,
     "end_time": "2023-05-25T14:58:58.985650",
     "exception": false,
     "start_time": "2023-05-25T14:58:58.337676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import random\n",
    " \n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    " \n",
    "\n",
    "from rich.progress import Progress, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn, track\n",
    "import time\n",
    "\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "same_seeds(42) \n",
    "\n",
    "source_transform = transforms.Compose([\n",
    "    # Turn RGB to grayscale. (Bacause Canny do not support RGB images.)\n",
    "    transforms.Grayscale(),\n",
    "    # cv2 do not support skimage.Image, so we transform it to np.array, \n",
    "    # and then adopt cv2.Canny algorithm.\n",
    "    transforms.Lambda(lambda x: cv2.Canny(np.array(x), 170, 300)),\n",
    "    # Transform np.array back to the skimage.Image.\n",
    "    transforms.ToPILImage(),\n",
    "    # 50% Horizontal Flip. (For Augmentation)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
    "    # if there's empty pixel after rotation.\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    # Transform to tensor for model inputs.\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "target_transform = transforms.Compose([\n",
    "    # Turn RGB to grayscale.\n",
    "    transforms.Grayscale(),\n",
    "    # Resize: size of source data is 32x32, thus we need to \n",
    "    #  enlarge the size of target data from 28x28 to 32x32。\n",
    "    transforms.Resize((32, 32)),\n",
    "    # 50% Horizontal Flip. (For Augmentation)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Rotate +- 15 degrees. (For Augmentation), and filled with zero \n",
    "    # if there's empty pixel after rotation.\n",
    "    transforms.RandomRotation(15, fill=(0,)),\n",
    "    # Transform to tensor for model inputs.\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    " \n",
    "source_dataset = ImageFolder('real_or_drawing/train_data', transform=source_transform)\n",
    "target_dataset = ImageFolder('real_or_drawing/test_data', transform=target_transform)\n",
    " \n",
    "source_dataloader = DataLoader(source_dataset, batch_size=32, shuffle=True)\n",
    "target_dataloader = DataLoader(target_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(target_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bf33077",
   "metadata": {
    "id": "hdwDEMrOycs5",
    "papermill": {
     "duration": 0.013476,
     "end_time": "2023-05-25T14:58:59.013318",
     "exception": false,
     "start_time": "2023-05-25T14:58:58.999842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model\n",
    "\n",
    "Feature Extractor: Classic VGG-like architecture\n",
    "\n",
    "Label Predictor / Domain Classifier: Linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20888f1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T14:58:59.042403Z",
     "iopub.status.busy": "2023-05-25T14:58:59.041677Z",
     "iopub.status.idle": "2023-05-25T14:58:59.055499Z",
     "shell.execute_reply": "2023-05-25T14:58:59.054698Z"
    },
    "id": "3uw2eP09z-pD",
    "papermill": {
     "duration": 0.030554,
     "end_time": "2023-05-25T14:58:59.057390",
     "exception": false,
     "start_time": "2023-05-25T14:58:59.026836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x).squeeze()\n",
    "        return x\n",
    "\n",
    "class LabelPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LabelPredictor, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        c = self.layer(h)\n",
    "        return c\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, h):\n",
    "        y = self.layer(h)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15799ad1",
   "metadata": {
    "id": "lxdBIPhF0Icb",
    "papermill": {
     "duration": 0.013285,
     "end_time": "2023-05-25T14:58:59.084127",
     "exception": false,
     "start_time": "2023-05-25T14:58:59.070842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "Here we use Adam as our optimizor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422fbace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T14:58:59.112923Z",
     "iopub.status.busy": "2023-05-25T14:58:59.112147Z",
     "iopub.status.idle": "2023-05-25T14:59:02.692188Z",
     "shell.execute_reply": "2023-05-25T14:59:02.691191Z"
    },
    "id": "hrxKelBy0PJ7",
    "papermill": {
     "duration": 3.596919,
     "end_time": "2023-05-25T14:59:02.694654",
     "exception": false,
     "start_time": "2023-05-25T14:58:59.097735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor().cuda()\n",
    "label_predictor = LabelPredictor().cuda()\n",
    "domain_classifier = DomainClassifier().cuda()\n",
    "\n",
    "class_criterion = nn.CrossEntropyLoss()\n",
    "domain_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_F = optim.Adam(feature_extractor.parameters())\n",
    "optimizer_C = optim.Adam(label_predictor.parameters())\n",
    "optimizer_D = optim.Adam(domain_classifier.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3de4dc15",
   "metadata": {
    "id": "xuAE4cqJ0itR",
    "papermill": {
     "duration": 0.014024,
     "end_time": "2023-05-25T14:59:02.724151",
     "exception": false,
     "start_time": "2023-05-25T14:59:02.710127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start Training\n",
    "\n",
    "\n",
    "## DaNN Implementation\n",
    "\n",
    "In the original paper, Gradient Reversal Layer is used.\n",
    "Feature Extractor, Label Predictor, and Domain Classifier are all trained at the same time. In this code, we train Domain Classifier first, and then train our Feature Extractor (same concept as Generator and Discriminator training process in GAN).\n",
    "\n",
    "## Reminder\n",
    "* Lambda, which controls the domain adversarial loss, is adaptive in the original paper. You can refer to [the original work](https://arxiv.org/pdf/1505.07818.pdf) . Here lambda is set to 0.1.\n",
    "* We do not have the label to target data, you can only evaluate your model by uploading your result to kaggle.:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df7ce2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T14:59:02.753390Z",
     "iopub.status.busy": "2023-05-25T14:59:02.753036Z",
     "iopub.status.idle": "2023-05-25T14:59:02.769336Z",
     "shell.execute_reply": "2023-05-25T14:59:02.768300Z"
    },
    "id": "lRAFFKvX0p9y",
    "outputId": "e2314aa7-0c4b-416a-913b-60298edfb865",
    "papermill": {
     "duration": 0.034556,
     "end_time": "2023-05-25T14:59:02.772285",
     "exception": false,
     "start_time": "2023-05-25T14:59:02.737729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_epochs = 2000\\nprint(\"start training\")\\nfor epoch in range(num_epochs):\\n    lamb = 2. / (1. + np.exp(-10 * epoch/ num_epochs))-1\\n    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb=lamb)\\n\\n    #progress.advance(epoch_tqdm, advance=1)\\n    if (epoch+1) % 100 == 0:\\n        torch.save(feature_extractor.state_dict(), f\\'extractor_model_{epoch+1}.bin\\')\\n        torch.save(label_predictor.state_dict(), f\\'predictor_model_{epoch+1}.bin\\')\\n\\n    torch.save(feature_extractor.state_dict(), f\\'extractor_model.bin\\')\\n    torch.save(label_predictor.state_dict(), f\\'predictor_model.bin\\')\\n    print(f\\'epoch {epoch+1}: train D loss: {train_D_loss}, train F loss: {train_F_loss}, acc {train_acc}\\')\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_epoch(source_dataloader, target_dataloader, lamb):\n",
    "    '''\n",
    "      Args:\n",
    "        source_dataloader: source data的dataloader\n",
    "        target_dataloader: target data的dataloader\n",
    "        lamb: control the balance of domain adaptatoin and classification.\n",
    "    '''\n",
    "\n",
    "    # D loss: Domain Classifier的loss\n",
    "    # F loss: Feature Extrator & Label Predictor的loss\n",
    "    running_D_loss, running_F_loss = 0.0, 0.0\n",
    "    total_hit, total_num = 0.0, 0.0\n",
    "    #batch_tqdm = progress.add_task(description=f\"batch_progress\", total=len(source_dataloader))\n",
    "\n",
    "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_dataloader, target_dataloader)):\n",
    "\n",
    "        source_data = source_data.cuda()\n",
    "        source_label = source_label.cuda()\n",
    "        target_data = target_data.cuda()\n",
    "        \n",
    "        # Mixed the source data and target data, or it'll mislead the running params\n",
    "        #   of batch_norm. (runnning mean/var of soucre and target data are different.)\n",
    "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
    "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
    "        # set domain label of source data to be 1.\n",
    "        domain_label[:source_data.shape[0]] = 1\n",
    "\n",
    "        # Step 1 : train domain classifier\n",
    "        feature = feature_extractor(mixed_data)\n",
    "        # We don't need to train feature extractor in step 1.\n",
    "        # Thus we detach the feature neuron to avoid backpropgation.\n",
    "        domain_logits = domain_classifier(feature.detach())\n",
    "        loss = domain_criterion(domain_logits, domain_label)\n",
    "        running_D_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Step 2 : train feature extractor and label classifier\n",
    "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
    "        domain_logits = domain_classifier(feature)\n",
    "        # loss = cross entropy of classification - lamb * domain binary cross entropy.\n",
    "        #  The reason why using subtraction is similar to generator loss in disciminator of GAN\n",
    "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
    "        running_F_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer_F.step()\n",
    "        optimizer_C.step()\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        optimizer_F.zero_grad()\n",
    "        optimizer_C.zero_grad()\n",
    "\n",
    "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
    "        total_num += source_data.shape[0]\n",
    "        #progress.advance(batch_tqdm, advance=1)\n",
    "\n",
    "    #progress.remove_task(batch_tqdm)\n",
    "    return running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n",
    "\n",
    "num_epochs = 2000\n",
    "print(\"start training\")\n",
    "for epoch in range(num_epochs):\n",
    "    lamb = 2. / (1. + np.exp(-10 * epoch/ num_epochs))-1\n",
    "    train_D_loss, train_F_loss, train_acc = train_epoch(source_dataloader, target_dataloader, lamb=lamb)\n",
    "\n",
    "    #progress.advance(epoch_tqdm, advance=1)\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        torch.save(feature_extractor.state_dict(), f'extractor_model_{epoch+1}.bin')\n",
    "        torch.save(label_predictor.state_dict(), f'predictor_model_{epoch+1}.bin')\n",
    "\n",
    "    torch.save(feature_extractor.state_dict(), f'extractor_model.bin')\n",
    "    torch.save(label_predictor.state_dict(), f'predictor_model.bin')\n",
    "    print(f'epoch {epoch+1}: train D loss: {train_D_loss}, train F loss: {train_F_loss}, acc {train_acc}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b16b15f2",
   "metadata": {
    "id": "o8_-0iSSje4w",
    "papermill": {
     "duration": 0.987201,
     "end_time": "2023-05-25T15:52:25.496362",
     "exception": false,
     "start_time": "2023-05-25T15:52:24.509161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference\n",
    "\n",
    "We use pandas to generate our csv file.\n",
    "\n",
    "BTW, the performance of the model trained for 200 epoches might be unstable. You can train for more epoches for a more stable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6ba6c3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T15:52:27.558305Z",
     "iopub.status.busy": "2023-05-25T15:52:27.557932Z",
     "iopub.status.idle": "2023-05-25T15:53:18.175569Z",
     "shell.execute_reply": "2023-05-25T15:53:18.174618Z"
    },
    "papermill": {
     "duration": 52.663658,
     "end_time": "2023-05-25T15:53:19.172250",
     "exception": false,
     "start_time": "2023-05-25T15:52:26.508592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get 81220 new data with psuedo label\n"
     ]
    }
   ],
   "source": [
    "#get psuedo label\n",
    "threshold = 1. - 1e-5\n",
    "pseudo_data, pseudo_label = torch.LongTensor([]).cuda(), torch.LongTensor([]).cuda()\n",
    "\n",
    "label_predictor.eval()\n",
    "feature_extractor.eval()\n",
    "\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "for i, (test_data, _) in enumerate(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        test_data = test_data.cuda()\n",
    "        class_logits = label_predictor(feature_extractor(test_data))\n",
    "        probs = softmax(class_logits)\n",
    "        prob = probs.max(dim=-1).values\n",
    "        x = torch.argmax(probs, dim=1)\n",
    "    \n",
    "    pseudo_data = torch.cat((pseudo_data, test_data[prob>threshold]), dim=0)\n",
    "    pseudo_label = torch.cat((pseudo_label, x[prob>threshold]), dim=0)\n",
    "\n",
    "print(f'get {len(pseudo_data)} new data with psuedo label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de514cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T15:53:21.237398Z",
     "iopub.status.busy": "2023-05-25T15:53:21.237038Z",
     "iopub.status.idle": "2023-05-25T15:53:21.243192Z",
     "shell.execute_reply": "2023-05-25T15:53:21.242302Z"
    },
    "papermill": {
     "duration": 1.012368,
     "end_time": "2023-05-25T15:53:21.245220",
     "exception": false,
     "start_time": "2023-05-25T15:53:20.232852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.data = x\n",
    "        self.label = y.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0fe1d1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T15:53:23.312171Z",
     "iopub.status.busy": "2023-05-25T15:53:23.311822Z",
     "iopub.status.idle": "2023-05-25T15:53:23.584742Z",
     "shell.execute_reply": "2023-05-25T15:53:23.583522Z"
    },
    "papermill": {
     "duration": 1.277559,
     "end_time": "2023-05-25T15:53:23.587190",
     "exception": false,
     "start_time": "2023-05-25T15:53:22.309631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81220\n",
      "81216\n"
     ]
    }
   ],
   "source": [
    "num = (len(pseudo_data)//32)*32 - len(pseudo_data)\n",
    "\n",
    "print(len(pseudo_data))\n",
    "print(len(pseudo_data[:num]))\n",
    "\n",
    "pseudo_dataset = PseudoDataset(pseudo_data[:num].cpu(), pseudo_label[:num].cpu())\n",
    "combine_dataset = ConcatDataset([pseudo_dataset, source_dataset])\n",
    "combine_dataloader = DataLoader(pseudo_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3da6b5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T15:53:25.685485Z",
     "iopub.status.busy": "2023-05-25T15:53:25.685120Z",
     "iopub.status.idle": "2023-05-25T17:18:12.008256Z",
     "shell.execute_reply": "2023-05-25T17:18:12.007210Z"
    },
    "papermill": {
     "duration": 5087.34093,
     "end_time": "2023-05-25T17:18:12.010777",
     "exception": false,
     "start_time": "2023-05-25T15:53:24.669847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchensemble\r\n",
      "  Downloading torchensemble-0.1.9-py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from torchensemble) (1.2.2)\r\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from torchensemble) (2.0.0)\r\n",
      "Requirement already satisfied: torchvision>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from torchensemble) (0.15.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->torchensemble) (1.23.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->torchensemble) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->torchensemble) (1.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->torchensemble) (1.9.3)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchensemble) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchensemble) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchensemble) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchensemble) (3.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->torchensemble) (3.11.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.2.2->torchensemble) (2.28.2)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.2.2->torchensemble) (9.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->torchensemble) (2.1.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.2.2->torchensemble) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.2.2->torchensemble) (1.26.15)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.2.2->torchensemble) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.2.2->torchensemble) (3.4)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->torchensemble) (1.3.0)\r\n",
      "Installing collected packages: torchensemble\r\n",
      "Successfully installed torchensemble-0.1.9\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: 000 | Epoch: 000 | Batch: 000 | Loss: 2.45317 | Correct: 4/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 100 | Loss: 0.69568 | Correct: 25/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 200 | Loss: 0.24945 | Correct: 28/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 300 | Loss: 0.35511 | Correct: 27/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 400 | Loss: 0.09521 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 500 | Loss: 0.22225 | Correct: 29/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 600 | Loss: 0.07719 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 700 | Loss: 0.11475 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 800 | Loss: 0.20984 | Correct: 29/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 900 | Loss: 0.19530 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1000 | Loss: 0.10951 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1100 | Loss: 0.04343 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1200 | Loss: 0.32652 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1300 | Loss: 0.07993 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1400 | Loss: 0.05591 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1500 | Loss: 0.14362 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1600 | Loss: 0.16907 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1700 | Loss: 0.07904 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1800 | Loss: 0.01972 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 1900 | Loss: 0.02227 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 2000 | Loss: 0.21249 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 2100 | Loss: 0.06159 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 2200 | Loss: 0.23574 | Correct: 29/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 2300 | Loss: 0.14967 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 2400 | Loss: 0.05561 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 000 | Batch: 2500 | Loss: 0.35336 | Correct: 27/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 000 | Loss: 2.69375 | Correct: 4/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 100 | Loss: 0.74744 | Correct: 24/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 200 | Loss: 0.15846 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 300 | Loss: 0.17220 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 400 | Loss: 0.16850 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 500 | Loss: 0.19913 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 600 | Loss: 0.04074 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 700 | Loss: 0.35752 | Correct: 29/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 800 | Loss: 0.06842 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 900 | Loss: 0.35593 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1000 | Loss: 0.26952 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1100 | Loss: 0.05058 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1200 | Loss: 0.08741 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1300 | Loss: 0.26186 | Correct: 29/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1400 | Loss: 0.25783 | Correct: 29/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1500 | Loss: 0.10998 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1600 | Loss: 0.23864 | Correct: 29/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1700 | Loss: 0.05485 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1800 | Loss: 0.05398 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 1900 | Loss: 0.11287 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 2000 | Loss: 0.01406 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 2100 | Loss: 0.15152 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 2200 | Loss: 0.14522 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 2300 | Loss: 0.01986 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 2400 | Loss: 0.01395 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 000 | Batch: 2500 | Loss: 0.08769 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 000 | Loss: 2.45741 | Correct: 3/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 100 | Loss: 0.32425 | Correct: 29/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 200 | Loss: 0.38995 | Correct: 28/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 300 | Loss: 0.28378 | Correct: 27/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 400 | Loss: 0.24320 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 500 | Loss: 0.29376 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 600 | Loss: 0.23220 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 700 | Loss: 0.06590 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 800 | Loss: 0.06289 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 900 | Loss: 0.16189 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1000 | Loss: 0.19867 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1100 | Loss: 0.11688 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1200 | Loss: 0.14137 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1300 | Loss: 0.02749 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1400 | Loss: 0.14209 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1500 | Loss: 0.32428 | Correct: 28/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1600 | Loss: 0.03580 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1700 | Loss: 0.09846 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1800 | Loss: 0.03028 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 1900 | Loss: 0.14622 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 2000 | Loss: 0.30138 | Correct: 28/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 2100 | Loss: 0.09385 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 2200 | Loss: 0.01987 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 2300 | Loss: 0.11219 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 2400 | Loss: 0.07589 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 000 | Batch: 2500 | Loss: 0.10087 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 000 | Loss: 2.52719 | Correct: 2/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 100 | Loss: 0.38931 | Correct: 27/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 200 | Loss: 0.48792 | Correct: 27/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 300 | Loss: 0.24880 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 400 | Loss: 0.60600 | Correct: 26/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 500 | Loss: 0.24403 | Correct: 29/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 600 | Loss: 0.18051 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 700 | Loss: 0.06931 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 800 | Loss: 0.27749 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 900 | Loss: 0.02291 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1000 | Loss: 0.13755 | Correct: 29/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1100 | Loss: 0.53186 | Correct: 27/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1200 | Loss: 0.21224 | Correct: 29/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1300 | Loss: 0.17176 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1400 | Loss: 0.02623 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1500 | Loss: 0.12911 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1600 | Loss: 0.23356 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1700 | Loss: 0.33262 | Correct: 29/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1800 | Loss: 0.07426 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 1900 | Loss: 0.01260 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 2000 | Loss: 0.04944 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 2100 | Loss: 0.03619 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 2200 | Loss: 0.47008 | Correct: 29/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 2300 | Loss: 0.08765 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 2400 | Loss: 0.10427 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 000 | Batch: 2500 | Loss: 0.01466 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 000 | Loss: 0.00452 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 100 | Loss: 0.12554 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 200 | Loss: 0.08990 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 300 | Loss: 0.01384 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 400 | Loss: 0.08901 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 500 | Loss: 0.06282 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 600 | Loss: 0.07144 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 700 | Loss: 0.70344 | Correct: 27/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 800 | Loss: 0.11667 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 900 | Loss: 0.20738 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1000 | Loss: 0.07863 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1100 | Loss: 0.03661 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1200 | Loss: 0.02383 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1300 | Loss: 0.04435 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1400 | Loss: 0.03570 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1500 | Loss: 0.16524 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1600 | Loss: 0.04811 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1700 | Loss: 0.01063 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1800 | Loss: 0.02364 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 1900 | Loss: 0.26555 | Correct: 29/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 2000 | Loss: 0.02491 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 2100 | Loss: 0.07286 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 2200 | Loss: 0.20572 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 2300 | Loss: 0.06625 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 2400 | Loss: 0.00215 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 001 | Batch: 2500 | Loss: 0.20579 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 000 | Loss: 0.04207 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 100 | Loss: 0.03114 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 200 | Loss: 0.36132 | Correct: 29/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 300 | Loss: 0.06850 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 400 | Loss: 0.00994 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 500 | Loss: 0.08025 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 600 | Loss: 0.12484 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 700 | Loss: 0.09086 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 800 | Loss: 0.12521 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 900 | Loss: 0.16278 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1000 | Loss: 0.04776 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1100 | Loss: 0.00824 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1200 | Loss: 0.00615 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1300 | Loss: 0.01031 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1400 | Loss: 0.01038 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1500 | Loss: 0.17626 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1600 | Loss: 0.03491 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1700 | Loss: 0.07251 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1800 | Loss: 0.08233 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 1900 | Loss: 0.16733 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 2000 | Loss: 0.12893 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 2100 | Loss: 0.04827 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 2200 | Loss: 0.06046 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 2300 | Loss: 0.02344 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 2400 | Loss: 0.16275 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 001 | Batch: 2500 | Loss: 0.07966 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 000 | Loss: 0.06025 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 100 | Loss: 0.13428 | Correct: 29/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 200 | Loss: 0.07696 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 300 | Loss: 0.24856 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 400 | Loss: 0.05378 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 500 | Loss: 0.01276 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 600 | Loss: 0.11136 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 700 | Loss: 0.01700 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 800 | Loss: 0.03184 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 900 | Loss: 0.37950 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1000 | Loss: 0.19770 | Correct: 28/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1100 | Loss: 0.05816 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1200 | Loss: 0.08600 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1300 | Loss: 0.01016 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1400 | Loss: 0.06112 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1500 | Loss: 0.01598 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1600 | Loss: 0.10663 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1700 | Loss: 0.09152 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1800 | Loss: 0.06365 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 1900 | Loss: 0.31473 | Correct: 27/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 2000 | Loss: 0.01423 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 2100 | Loss: 0.06773 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 2200 | Loss: 0.02231 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 2300 | Loss: 0.13543 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 2400 | Loss: 0.03684 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 001 | Batch: 2500 | Loss: 0.19019 | Correct: 29/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 000 | Loss: 0.37201 | Correct: 29/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 100 | Loss: 0.02584 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 200 | Loss: 0.11124 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 300 | Loss: 0.03884 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 400 | Loss: 0.04176 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 500 | Loss: 0.12913 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 600 | Loss: 0.32423 | Correct: 27/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 700 | Loss: 0.06645 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 800 | Loss: 0.02139 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 900 | Loss: 0.09176 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1000 | Loss: 0.23800 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1100 | Loss: 0.02889 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1200 | Loss: 0.06373 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1300 | Loss: 0.05209 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1400 | Loss: 0.07425 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1500 | Loss: 0.01459 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1600 | Loss: 0.05486 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1700 | Loss: 0.03346 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1800 | Loss: 0.02204 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 1900 | Loss: 0.29317 | Correct: 29/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 2000 | Loss: 0.01592 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 2100 | Loss: 0.00762 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 2200 | Loss: 0.11269 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 2300 | Loss: 0.10652 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 2400 | Loss: 0.02859 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 001 | Batch: 2500 | Loss: 0.08886 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 000 | Loss: 0.00467 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 100 | Loss: 0.10682 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 200 | Loss: 0.06386 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 300 | Loss: 0.02795 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 400 | Loss: 0.19787 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 500 | Loss: 0.00372 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 600 | Loss: 0.07686 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 700 | Loss: 0.05185 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 800 | Loss: 0.00729 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 900 | Loss: 0.09182 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1000 | Loss: 0.08398 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1100 | Loss: 0.03486 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1200 | Loss: 0.03287 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1300 | Loss: 0.26148 | Correct: 29/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1400 | Loss: 0.24148 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1500 | Loss: 0.01103 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1600 | Loss: 0.14711 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1700 | Loss: 0.06964 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1800 | Loss: 0.00278 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 1900 | Loss: 0.00533 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 2000 | Loss: 0.00958 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 2100 | Loss: 0.00416 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 2200 | Loss: 0.14540 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 2300 | Loss: 0.31927 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 2400 | Loss: 0.24977 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 002 | Batch: 2500 | Loss: 0.04178 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 000 | Loss: 0.00551 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 100 | Loss: 0.03476 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 200 | Loss: 0.03831 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 300 | Loss: 0.03818 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 400 | Loss: 0.00545 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 500 | Loss: 0.00361 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 600 | Loss: 0.00943 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 700 | Loss: 0.06961 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 800 | Loss: 0.09340 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 900 | Loss: 0.01225 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1000 | Loss: 0.04973 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1100 | Loss: 0.02500 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1200 | Loss: 0.00695 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1300 | Loss: 0.03458 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1400 | Loss: 0.06103 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1500 | Loss: 0.02327 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1600 | Loss: 0.39211 | Correct: 29/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1700 | Loss: 0.02610 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1800 | Loss: 0.18027 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 1900 | Loss: 0.01300 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 2000 | Loss: 0.00390 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 2100 | Loss: 0.10602 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 2200 | Loss: 0.17475 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 2300 | Loss: 0.00633 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 2400 | Loss: 0.01579 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 002 | Batch: 2500 | Loss: 0.03615 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 000 | Loss: 0.02700 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 100 | Loss: 0.00562 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 200 | Loss: 0.15955 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 300 | Loss: 0.16476 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 400 | Loss: 0.09953 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 500 | Loss: 0.04821 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 600 | Loss: 0.09626 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 700 | Loss: 0.04736 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 800 | Loss: 0.05175 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 900 | Loss: 0.10297 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1000 | Loss: 0.00769 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1100 | Loss: 0.08000 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1200 | Loss: 0.03047 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1300 | Loss: 0.00892 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1400 | Loss: 0.00362 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1500 | Loss: 0.04388 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1600 | Loss: 0.04383 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1700 | Loss: 0.02414 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1800 | Loss: 0.27584 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 1900 | Loss: 0.00701 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 2000 | Loss: 0.00669 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 2100 | Loss: 0.02390 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 2200 | Loss: 0.00721 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 2300 | Loss: 0.00622 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 2400 | Loss: 0.01177 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 002 | Batch: 2500 | Loss: 0.09879 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 000 | Loss: 0.01358 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 100 | Loss: 0.01834 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 200 | Loss: 0.05513 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 300 | Loss: 0.01455 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 400 | Loss: 0.00630 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 500 | Loss: 0.08704 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 600 | Loss: 0.01029 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 700 | Loss: 0.05468 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 800 | Loss: 0.05816 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 900 | Loss: 0.01053 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1000 | Loss: 0.01270 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1100 | Loss: 0.00936 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1200 | Loss: 0.00608 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1300 | Loss: 0.02327 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1400 | Loss: 0.03010 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1500 | Loss: 0.04930 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1600 | Loss: 0.14324 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1700 | Loss: 0.01189 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1800 | Loss: 0.14879 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 1900 | Loss: 0.19146 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 2000 | Loss: 0.04523 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 2100 | Loss: 0.00998 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 2200 | Loss: 0.12216 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 2300 | Loss: 0.04312 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 2400 | Loss: 0.00366 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 002 | Batch: 2500 | Loss: 0.00367 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 000 | Loss: 0.00301 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 100 | Loss: 0.13136 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 200 | Loss: 0.05488 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 300 | Loss: 0.00621 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 400 | Loss: 0.04147 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 500 | Loss: 0.06575 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 600 | Loss: 0.01156 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 700 | Loss: 0.00800 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 800 | Loss: 0.01104 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 900 | Loss: 0.00326 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1000 | Loss: 0.00175 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1100 | Loss: 0.11044 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1200 | Loss: 0.22215 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1300 | Loss: 0.16284 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1400 | Loss: 0.14868 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1500 | Loss: 0.02130 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1600 | Loss: 0.04611 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1700 | Loss: 0.00999 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1800 | Loss: 0.00615 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 1900 | Loss: 0.14454 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 2000 | Loss: 0.00844 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 2100 | Loss: 0.09680 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 2200 | Loss: 0.03707 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 2300 | Loss: 0.05043 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 2400 | Loss: 0.00451 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 003 | Batch: 2500 | Loss: 0.00462 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 000 | Loss: 0.00690 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 100 | Loss: 0.05385 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 200 | Loss: 0.12849 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 300 | Loss: 0.09657 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 400 | Loss: 0.00920 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 500 | Loss: 0.02333 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 600 | Loss: 0.06099 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 700 | Loss: 0.20028 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 800 | Loss: 0.01284 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 900 | Loss: 0.00913 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1000 | Loss: 0.10805 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1100 | Loss: 0.03390 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1200 | Loss: 0.00430 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1300 | Loss: 0.03222 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1400 | Loss: 0.01024 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1500 | Loss: 0.00170 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1600 | Loss: 0.05126 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1700 | Loss: 0.06410 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1800 | Loss: 0.02462 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 1900 | Loss: 0.00336 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 2000 | Loss: 0.00405 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 2100 | Loss: 0.10081 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 2200 | Loss: 0.00374 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 2300 | Loss: 0.24239 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 2400 | Loss: 0.00374 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 003 | Batch: 2500 | Loss: 0.09620 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 000 | Loss: 0.01143 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 100 | Loss: 0.04125 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 200 | Loss: 0.01408 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 300 | Loss: 0.09340 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 400 | Loss: 0.00103 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 500 | Loss: 0.02518 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 600 | Loss: 0.16921 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 700 | Loss: 0.04329 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 800 | Loss: 0.07583 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 900 | Loss: 0.13948 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1000 | Loss: 0.00497 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1100 | Loss: 0.02150 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1200 | Loss: 0.05202 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1300 | Loss: 0.04009 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1400 | Loss: 0.00504 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1500 | Loss: 0.08247 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1600 | Loss: 0.01042 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1700 | Loss: 0.11216 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1800 | Loss: 0.00734 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 1900 | Loss: 0.02735 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 2000 | Loss: 0.06335 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 2100 | Loss: 0.00287 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 2200 | Loss: 0.01618 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 2300 | Loss: 0.00727 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 2400 | Loss: 0.09812 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 003 | Batch: 2500 | Loss: 0.00260 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 000 | Loss: 0.21646 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 100 | Loss: 0.01263 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 200 | Loss: 0.00870 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 300 | Loss: 0.16398 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 400 | Loss: 0.00275 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 500 | Loss: 0.00342 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 600 | Loss: 0.07828 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 700 | Loss: 0.13187 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 800 | Loss: 0.05071 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 900 | Loss: 0.01791 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1000 | Loss: 0.01925 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1100 | Loss: 0.00451 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1200 | Loss: 0.12592 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1300 | Loss: 0.04762 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1400 | Loss: 0.08348 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1500 | Loss: 0.04378 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1600 | Loss: 0.04665 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1700 | Loss: 0.03489 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1800 | Loss: 0.06013 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 1900 | Loss: 0.00429 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 2000 | Loss: 0.00476 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 2100 | Loss: 0.06121 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 2200 | Loss: 0.06054 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 2300 | Loss: 0.07284 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 2400 | Loss: 0.01777 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 003 | Batch: 2500 | Loss: 0.19656 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 000 | Loss: 0.03795 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 100 | Loss: 0.00112 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 200 | Loss: 0.00106 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 300 | Loss: 0.00989 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 400 | Loss: 0.01620 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 500 | Loss: 0.08850 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 600 | Loss: 0.01957 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 700 | Loss: 0.04848 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 800 | Loss: 0.00111 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 900 | Loss: 0.02642 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1000 | Loss: 0.23059 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1100 | Loss: 0.00398 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1200 | Loss: 0.04118 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1300 | Loss: 0.14546 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1400 | Loss: 0.00719 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1500 | Loss: 0.08026 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1600 | Loss: 0.00586 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1700 | Loss: 0.00412 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1800 | Loss: 0.02587 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 1900 | Loss: 0.10052 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 2000 | Loss: 0.00141 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 2100 | Loss: 0.00787 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 2200 | Loss: 0.01163 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 2300 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 2400 | Loss: 0.04367 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 004 | Batch: 2500 | Loss: 0.00214 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 000 | Loss: 0.02786 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 100 | Loss: 0.00235 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 200 | Loss: 0.10202 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 300 | Loss: 0.04261 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 400 | Loss: 0.00259 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 500 | Loss: 0.00597 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 600 | Loss: 0.29711 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 700 | Loss: 0.00268 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 800 | Loss: 0.00111 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 900 | Loss: 0.04652 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1000 | Loss: 0.00822 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1100 | Loss: 0.06419 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1200 | Loss: 0.02321 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1300 | Loss: 0.02506 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1400 | Loss: 0.01248 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1500 | Loss: 0.00806 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1600 | Loss: 0.00533 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1700 | Loss: 0.07839 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1800 | Loss: 0.00419 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 1900 | Loss: 0.00773 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 2000 | Loss: 0.01837 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 2100 | Loss: 0.00787 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 2200 | Loss: 0.05614 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 2300 | Loss: 0.02225 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 2400 | Loss: 0.07117 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 004 | Batch: 2500 | Loss: 0.02552 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 000 | Loss: 0.00169 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 100 | Loss: 0.00596 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 200 | Loss: 0.02367 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 300 | Loss: 0.04979 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 400 | Loss: 0.00331 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 500 | Loss: 0.01325 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 600 | Loss: 0.00079 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 700 | Loss: 0.01355 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 800 | Loss: 0.00121 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 900 | Loss: 0.00397 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1000 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1100 | Loss: 0.01905 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1200 | Loss: 0.00968 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1300 | Loss: 0.06129 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1400 | Loss: 0.00155 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1500 | Loss: 0.00863 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1600 | Loss: 0.00568 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1700 | Loss: 0.01128 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1800 | Loss: 0.00241 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 1900 | Loss: 0.06772 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 2000 | Loss: 0.00495 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 2100 | Loss: 0.00639 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 2200 | Loss: 0.00214 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 2300 | Loss: 0.01689 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 2400 | Loss: 0.03080 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 004 | Batch: 2500 | Loss: 0.00664 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 000 | Loss: 0.01125 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 100 | Loss: 0.00261 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 200 | Loss: 0.00455 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 300 | Loss: 0.06663 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 400 | Loss: 0.01869 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 500 | Loss: 0.00190 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 600 | Loss: 0.07024 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 700 | Loss: 0.07809 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 800 | Loss: 0.02042 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 900 | Loss: 0.01776 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1000 | Loss: 0.01249 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1100 | Loss: 0.00607 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1200 | Loss: 0.12273 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1300 | Loss: 0.00423 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1400 | Loss: 0.00148 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1500 | Loss: 0.11460 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1600 | Loss: 0.00752 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1700 | Loss: 0.00939 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1800 | Loss: 0.11658 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 1900 | Loss: 0.01185 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 2000 | Loss: 0.00608 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 2100 | Loss: 0.32746 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 2200 | Loss: 0.01071 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 2300 | Loss: 0.00466 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 2400 | Loss: 0.00292 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 004 | Batch: 2500 | Loss: 0.01862 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 000 | Loss: 0.00290 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 100 | Loss: 0.07035 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 200 | Loss: 0.00405 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 300 | Loss: 0.14374 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 400 | Loss: 0.05743 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 500 | Loss: 0.00120 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 600 | Loss: 0.01034 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 700 | Loss: 0.00645 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 800 | Loss: 0.04484 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 900 | Loss: 0.00597 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1000 | Loss: 0.00744 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1100 | Loss: 0.02988 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1200 | Loss: 0.00080 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1300 | Loss: 0.01327 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1400 | Loss: 0.00573 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1500 | Loss: 0.00096 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1600 | Loss: 0.01287 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1700 | Loss: 0.00839 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1800 | Loss: 0.06013 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 1900 | Loss: 0.00184 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 2000 | Loss: 0.00027 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 2100 | Loss: 0.00174 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 2200 | Loss: 0.10707 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 2300 | Loss: 0.04326 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 2400 | Loss: 0.00573 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 005 | Batch: 2500 | Loss: 0.00648 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 000 | Loss: 0.00137 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 100 | Loss: 0.00232 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 200 | Loss: 0.03197 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 300 | Loss: 0.05547 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 400 | Loss: 0.03640 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 500 | Loss: 0.00476 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 600 | Loss: 0.00141 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 700 | Loss: 0.06056 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 800 | Loss: 0.00428 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 900 | Loss: 0.02106 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1000 | Loss: 0.07500 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1100 | Loss: 0.05230 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1200 | Loss: 0.00338 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1300 | Loss: 0.06055 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1400 | Loss: 0.03702 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1500 | Loss: 0.29391 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1600 | Loss: 0.00137 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1700 | Loss: 0.00148 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1800 | Loss: 0.00102 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 1900 | Loss: 0.00804 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 2000 | Loss: 0.00173 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 2100 | Loss: 0.00576 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 2200 | Loss: 0.00429 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 2300 | Loss: 0.00230 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 2400 | Loss: 0.00187 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 005 | Batch: 2500 | Loss: 0.00195 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 000 | Loss: 0.08313 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 100 | Loss: 0.01339 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 200 | Loss: 0.07716 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 300 | Loss: 0.06639 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 400 | Loss: 0.00081 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 500 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 600 | Loss: 0.02540 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 700 | Loss: 0.02884 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 800 | Loss: 0.00071 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 900 | Loss: 0.01561 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1000 | Loss: 0.08608 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1100 | Loss: 0.04713 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1200 | Loss: 0.00228 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1300 | Loss: 0.01062 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1400 | Loss: 0.01354 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1500 | Loss: 0.00209 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1600 | Loss: 0.00466 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1700 | Loss: 0.00117 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1800 | Loss: 0.02526 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 1900 | Loss: 0.00880 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 2000 | Loss: 0.02224 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 2100 | Loss: 0.00729 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 2200 | Loss: 0.00740 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 2300 | Loss: 0.00090 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 2400 | Loss: 0.00127 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 005 | Batch: 2500 | Loss: 0.03387 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 000 | Loss: 0.02259 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 100 | Loss: 0.00049 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 200 | Loss: 0.07964 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 300 | Loss: 0.00448 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 400 | Loss: 0.00125 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 500 | Loss: 0.07820 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 600 | Loss: 0.00778 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 700 | Loss: 0.00670 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 800 | Loss: 0.00711 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 900 | Loss: 0.00967 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1000 | Loss: 0.02528 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1100 | Loss: 0.02466 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1200 | Loss: 0.00843 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1300 | Loss: 0.00420 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1400 | Loss: 0.06750 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1500 | Loss: 0.01475 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1600 | Loss: 0.01796 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1700 | Loss: 0.08681 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1800 | Loss: 0.00237 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 1900 | Loss: 0.26380 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 2000 | Loss: 0.18286 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 2100 | Loss: 0.00324 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 2200 | Loss: 0.16262 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 2300 | Loss: 0.00043 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 2400 | Loss: 0.00239 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 005 | Batch: 2500 | Loss: 0.00823 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 000 | Loss: 0.00184 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 100 | Loss: 0.00221 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 200 | Loss: 0.05388 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 300 | Loss: 0.00365 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 400 | Loss: 0.01471 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 500 | Loss: 0.05476 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 600 | Loss: 0.01685 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 700 | Loss: 0.11107 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 800 | Loss: 0.02063 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 900 | Loss: 0.06454 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1000 | Loss: 0.00455 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1100 | Loss: 0.00396 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1200 | Loss: 0.04413 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1300 | Loss: 0.12012 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1400 | Loss: 0.01677 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1500 | Loss: 0.01006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1600 | Loss: 0.01469 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1700 | Loss: 0.07753 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1800 | Loss: 0.03595 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 1900 | Loss: 0.02829 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 2000 | Loss: 0.00105 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 2100 | Loss: 0.07901 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 2200 | Loss: 0.11514 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 2300 | Loss: 0.02010 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 2400 | Loss: 0.00052 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 006 | Batch: 2500 | Loss: 0.00083 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 000 | Loss: 0.00368 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 100 | Loss: 0.07142 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 200 | Loss: 0.01174 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 300 | Loss: 0.07151 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 400 | Loss: 0.24777 | Correct: 29/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 500 | Loss: 0.00085 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 600 | Loss: 0.01068 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 700 | Loss: 0.00274 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 800 | Loss: 0.00401 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 900 | Loss: 0.00086 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1000 | Loss: 0.07041 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1100 | Loss: 0.00090 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1200 | Loss: 0.00949 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1300 | Loss: 0.00234 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1400 | Loss: 0.00023 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1500 | Loss: 0.37219 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1600 | Loss: 0.04398 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1700 | Loss: 0.02894 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1800 | Loss: 0.00237 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 1900 | Loss: 0.06697 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 2000 | Loss: 0.00383 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 2100 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 2200 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 2300 | Loss: 0.00415 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 2400 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 006 | Batch: 2500 | Loss: 0.00156 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 000 | Loss: 0.05293 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 100 | Loss: 0.00622 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 200 | Loss: 0.00356 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 300 | Loss: 0.00899 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 400 | Loss: 0.00649 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 500 | Loss: 0.00422 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 600 | Loss: 0.00788 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 700 | Loss: 0.23834 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 800 | Loss: 0.03875 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 900 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1000 | Loss: 0.00082 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1100 | Loss: 0.00672 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1200 | Loss: 0.12566 | Correct: 30/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1300 | Loss: 0.00069 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1400 | Loss: 0.00527 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1500 | Loss: 0.00492 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1600 | Loss: 0.00345 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1700 | Loss: 0.01811 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1800 | Loss: 0.00080 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 1900 | Loss: 0.00053 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 2000 | Loss: 0.17758 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 2100 | Loss: 0.00065 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 2200 | Loss: 0.01332 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 2300 | Loss: 0.00063 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 2400 | Loss: 0.02610 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 006 | Batch: 2500 | Loss: 0.01848 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 000 | Loss: 0.00575 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 100 | Loss: 0.00213 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 200 | Loss: 0.00125 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 300 | Loss: 0.01309 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 400 | Loss: 0.02856 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 500 | Loss: 0.00278 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 600 | Loss: 0.00060 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 700 | Loss: 0.00054 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 800 | Loss: 0.00443 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 900 | Loss: 0.00329 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1000 | Loss: 0.00195 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1100 | Loss: 0.17131 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1200 | Loss: 0.00101 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1300 | Loss: 0.06937 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1400 | Loss: 0.00212 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1500 | Loss: 0.01779 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1600 | Loss: 0.05851 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1700 | Loss: 0.00380 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1800 | Loss: 0.00066 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 1900 | Loss: 0.00104 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 2000 | Loss: 0.03116 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 2100 | Loss: 0.08152 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 2200 | Loss: 0.00082 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 2300 | Loss: 0.00141 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 2400 | Loss: 0.00034 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 006 | Batch: 2500 | Loss: 0.02907 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 000 | Loss: 0.00551 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 100 | Loss: 0.00575 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 200 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 300 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 400 | Loss: 0.04301 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 500 | Loss: 0.00298 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 600 | Loss: 0.00048 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 700 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 800 | Loss: 0.00047 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 900 | Loss: 0.01925 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1000 | Loss: 0.00032 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1100 | Loss: 0.02810 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1200 | Loss: 0.00208 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1300 | Loss: 0.00186 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1400 | Loss: 0.03667 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1500 | Loss: 0.00634 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1600 | Loss: 0.00100 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1700 | Loss: 0.08364 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1800 | Loss: 0.08781 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 1900 | Loss: 0.00052 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 2000 | Loss: 0.02223 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 2100 | Loss: 0.02149 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 2200 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 2300 | Loss: 0.00480 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 2400 | Loss: 0.01120 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 007 | Batch: 2500 | Loss: 0.01806 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 000 | Loss: 0.04367 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 100 | Loss: 0.02173 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 200 | Loss: 0.00198 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 300 | Loss: 0.00109 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 400 | Loss: 0.00064 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 500 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 600 | Loss: 0.01540 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 700 | Loss: 0.00193 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 800 | Loss: 0.00920 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 900 | Loss: 0.00157 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1000 | Loss: 0.06401 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1100 | Loss: 0.00195 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1200 | Loss: 0.02780 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1300 | Loss: 0.04122 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1400 | Loss: 0.00632 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1500 | Loss: 0.03003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1600 | Loss: 0.00230 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1700 | Loss: 0.00043 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1800 | Loss: 0.01295 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 1900 | Loss: 0.02911 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 2000 | Loss: 0.00154 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 2100 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 2200 | Loss: 0.02463 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 2300 | Loss: 0.06684 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 2400 | Loss: 0.00036 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 007 | Batch: 2500 | Loss: 0.07222 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 000 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 100 | Loss: 0.04080 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 200 | Loss: 0.00754 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 300 | Loss: 0.00144 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 400 | Loss: 0.00088 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 500 | Loss: 0.10997 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 600 | Loss: 0.01603 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 700 | Loss: 0.01332 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 800 | Loss: 0.00106 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 900 | Loss: 0.00069 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1000 | Loss: 0.03725 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1100 | Loss: 0.04768 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1200 | Loss: 0.01088 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1300 | Loss: 0.00023 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1400 | Loss: 0.12681 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1500 | Loss: 0.00048 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1600 | Loss: 0.04593 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1700 | Loss: 0.00077 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1800 | Loss: 0.00259 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 1900 | Loss: 0.00341 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 2000 | Loss: 0.00102 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 2100 | Loss: 0.01460 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 2200 | Loss: 0.00261 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 2300 | Loss: 0.00303 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 2400 | Loss: 0.00232 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 007 | Batch: 2500 | Loss: 0.06995 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 000 | Loss: 0.00063 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 100 | Loss: 0.07530 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 200 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 300 | Loss: 0.00604 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 400 | Loss: 0.01285 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 500 | Loss: 0.00046 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 600 | Loss: 0.00389 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 700 | Loss: 0.00195 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 800 | Loss: 0.08551 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 900 | Loss: 0.00209 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1000 | Loss: 0.05923 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1100 | Loss: 0.01821 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1200 | Loss: 0.11535 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1300 | Loss: 0.00232 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1400 | Loss: 0.00644 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1500 | Loss: 0.00065 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1600 | Loss: 0.04211 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1700 | Loss: 0.00152 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1800 | Loss: 0.00158 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 1900 | Loss: 0.00476 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 2000 | Loss: 0.00169 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 2100 | Loss: 0.00243 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 2200 | Loss: 0.00111 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 2300 | Loss: 0.00039 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 2400 | Loss: 0.02532 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 007 | Batch: 2500 | Loss: 0.00180 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 000 | Loss: 0.00330 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 100 | Loss: 0.00113 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 200 | Loss: 0.00037 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 300 | Loss: 0.05589 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 400 | Loss: 0.14498 | Correct: 30/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 500 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 600 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 700 | Loss: 0.14932 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 800 | Loss: 0.00698 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 900 | Loss: 0.00720 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1000 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1100 | Loss: 0.14465 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1200 | Loss: 0.00392 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1300 | Loss: 0.00152 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1400 | Loss: 0.00195 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1500 | Loss: 0.00823 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1600 | Loss: 0.00031 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1700 | Loss: 0.01022 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1800 | Loss: 0.04611 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 1900 | Loss: 0.00451 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 2000 | Loss: 0.04408 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 2100 | Loss: 0.00309 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 2200 | Loss: 0.01107 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 2300 | Loss: 0.00260 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 2400 | Loss: 0.02659 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 008 | Batch: 2500 | Loss: 0.03517 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 000 | Loss: 0.00052 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 100 | Loss: 0.00705 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 200 | Loss: 0.14717 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 300 | Loss: 0.03270 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 400 | Loss: 0.00023 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 500 | Loss: 0.00106 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 600 | Loss: 0.04604 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 700 | Loss: 0.00019 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 800 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 900 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1000 | Loss: 0.00056 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1100 | Loss: 0.02971 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1200 | Loss: 0.00175 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1300 | Loss: 0.00182 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1400 | Loss: 0.01852 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1500 | Loss: 0.01025 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1600 | Loss: 0.00044 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1700 | Loss: 0.01390 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1800 | Loss: 0.10714 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 1900 | Loss: 0.00025 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 2000 | Loss: 0.00368 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 2100 | Loss: 0.00033 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 2200 | Loss: 0.01101 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 2300 | Loss: 0.03534 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 2400 | Loss: 0.00067 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 008 | Batch: 2500 | Loss: 0.00039 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 000 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 100 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 200 | Loss: 0.00018 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 300 | Loss: 0.00045 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 400 | Loss: 0.01445 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 500 | Loss: 0.00053 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 600 | Loss: 0.00266 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 700 | Loss: 0.00068 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 800 | Loss: 0.02984 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 900 | Loss: 0.00266 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1000 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1100 | Loss: 0.01661 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1200 | Loss: 0.01737 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1300 | Loss: 0.00364 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1500 | Loss: 0.00039 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1600 | Loss: 0.00210 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1700 | Loss: 0.02606 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1800 | Loss: 0.06161 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 1900 | Loss: 0.00654 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 2000 | Loss: 0.00151 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 2100 | Loss: 0.00212 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 2200 | Loss: 0.06251 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 2300 | Loss: 0.01610 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 2400 | Loss: 0.00080 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 008 | Batch: 2500 | Loss: 0.00105 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 000 | Loss: 0.00042 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 100 | Loss: 0.00038 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 200 | Loss: 0.00271 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 400 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 500 | Loss: 0.00218 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 600 | Loss: 0.00061 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 700 | Loss: 0.00514 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 800 | Loss: 0.00108 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 900 | Loss: 0.00037 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1000 | Loss: 0.00065 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1100 | Loss: 0.00073 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1200 | Loss: 0.01807 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1300 | Loss: 0.00030 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1400 | Loss: 0.00141 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1500 | Loss: 0.00405 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1600 | Loss: 0.00315 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1700 | Loss: 0.21893 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1800 | Loss: 0.00786 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 1900 | Loss: 0.00507 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 2000 | Loss: 0.15178 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 2100 | Loss: 0.04063 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 2200 | Loss: 0.00176 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 2300 | Loss: 0.00302 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 2400 | Loss: 0.00544 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 008 | Batch: 2500 | Loss: 0.00103 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 000 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 100 | Loss: 0.00318 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 200 | Loss: 0.00134 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 300 | Loss: 0.00109 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 400 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 500 | Loss: 0.00077 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 600 | Loss: 0.00199 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 700 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 800 | Loss: 0.02286 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 900 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1000 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1100 | Loss: 0.00106 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1200 | Loss: 0.05190 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1300 | Loss: 0.02469 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1400 | Loss: 0.00836 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1500 | Loss: 0.00106 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1600 | Loss: 0.00197 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1700 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1800 | Loss: 0.00100 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 1900 | Loss: 0.05610 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 2000 | Loss: 0.00068 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 2100 | Loss: 0.00279 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 2200 | Loss: 0.00062 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 2300 | Loss: 0.04340 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 2400 | Loss: 0.00488 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 009 | Batch: 2500 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 000 | Loss: 0.00164 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 100 | Loss: 0.00062 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 200 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 300 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 400 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 500 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 600 | Loss: 0.00294 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 700 | Loss: 0.00327 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 800 | Loss: 0.02489 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 900 | Loss: 0.07000 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1000 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1100 | Loss: 0.00240 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1200 | Loss: 0.00018 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1300 | Loss: 0.00121 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1400 | Loss: 0.00111 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1500 | Loss: 0.00035 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1600 | Loss: 0.03400 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1700 | Loss: 0.00077 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1800 | Loss: 0.00418 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 1900 | Loss: 0.00174 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 2000 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 2100 | Loss: 0.00040 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 2200 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 2300 | Loss: 0.34742 | Correct: 30/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 2400 | Loss: 0.00198 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 009 | Batch: 2500 | Loss: 0.00080 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 000 | Loss: 0.00046 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 100 | Loss: 0.00055 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 200 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 300 | Loss: 0.10579 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 400 | Loss: 0.00706 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 500 | Loss: 0.00018 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 600 | Loss: 0.04730 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 700 | Loss: 0.00548 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 800 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 900 | Loss: 0.00054 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1000 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1100 | Loss: 0.00034 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1200 | Loss: 0.00124 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1300 | Loss: 0.00668 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1400 | Loss: 0.00109 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1500 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1700 | Loss: 0.00027 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1800 | Loss: 0.00036 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 1900 | Loss: 0.00210 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 2000 | Loss: 0.03657 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 2100 | Loss: 0.00032 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 2200 | Loss: 0.00091 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 2300 | Loss: 0.02171 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 2400 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 009 | Batch: 2500 | Loss: 0.00241 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 000 | Loss: 0.02394 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 100 | Loss: 0.00025 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 200 | Loss: 0.00349 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 300 | Loss: 0.00038 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 400 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 500 | Loss: 0.00082 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 600 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 700 | Loss: 0.00098 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 800 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 900 | Loss: 0.00139 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1000 | Loss: 0.01893 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1100 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1200 | Loss: 0.00147 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1300 | Loss: 0.00086 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1400 | Loss: 0.00125 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1500 | Loss: 0.00064 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1600 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1700 | Loss: 0.00134 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1800 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 1900 | Loss: 0.02580 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 2000 | Loss: 0.00031 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 2100 | Loss: 0.00044 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 2200 | Loss: 0.00115 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 2300 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 2400 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 009 | Batch: 2500 | Loss: 0.00257 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 000 | Loss: 0.03804 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 100 | Loss: 0.00039 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 200 | Loss: 0.00308 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 300 | Loss: 0.00038 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 400 | Loss: 0.00068 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 500 | Loss: 0.04721 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 600 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 700 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 800 | Loss: 0.00421 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 900 | Loss: 0.00300 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1000 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1100 | Loss: 0.00025 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1200 | Loss: 0.00340 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1300 | Loss: 0.00277 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1400 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1500 | Loss: 0.00090 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1600 | Loss: 0.00026 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1700 | Loss: 0.00151 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1800 | Loss: 0.00033 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 1900 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 2000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 2100 | Loss: 0.00398 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 2200 | Loss: 0.00036 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 2300 | Loss: 0.06302 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 2400 | Loss: 0.00033 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 010 | Batch: 2500 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 000 | Loss: 0.00186 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 100 | Loss: 0.00054 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 200 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 300 | Loss: 0.00475 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 400 | Loss: 0.00056 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 500 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 600 | Loss: 0.17959 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 800 | Loss: 0.00056 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1000 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1100 | Loss: 0.00018 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1400 | Loss: 0.03167 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1500 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1600 | Loss: 0.07376 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1700 | Loss: 0.00135 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1800 | Loss: 0.00167 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 1900 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 2000 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 2100 | Loss: 0.07672 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 2200 | Loss: 0.00090 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 2400 | Loss: 0.04361 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 010 | Batch: 2500 | Loss: 0.06581 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 000 | Loss: 0.00043 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 100 | Loss: 0.00192 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 200 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 300 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 400 | Loss: 0.08880 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 500 | Loss: 0.00062 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 600 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 700 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 800 | Loss: 0.00103 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 900 | Loss: 0.05837 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1000 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1100 | Loss: 0.00111 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1200 | Loss: 0.00045 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1300 | Loss: 0.00302 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1500 | Loss: 0.00475 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1600 | Loss: 0.00448 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1700 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1800 | Loss: 0.00065 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 1900 | Loss: 0.00057 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 2000 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 2100 | Loss: 0.00094 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 2200 | Loss: 0.00445 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 2300 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 2400 | Loss: 0.00124 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 010 | Batch: 2500 | Loss: 0.00383 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 000 | Loss: 0.00239 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 100 | Loss: 0.00712 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 200 | Loss: 0.00623 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 300 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 400 | Loss: 0.00218 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 500 | Loss: 0.00902 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 600 | Loss: 0.00024 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 700 | Loss: 0.00047 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 800 | Loss: 0.08504 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 900 | Loss: 0.00060 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1000 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1100 | Loss: 0.00024 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1200 | Loss: 0.01371 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1300 | Loss: 0.00518 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1400 | Loss: 0.24022 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1500 | Loss: 0.00449 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1600 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1800 | Loss: 0.00439 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 1900 | Loss: 0.00219 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 2000 | Loss: 0.38469 | Correct: 30/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 2100 | Loss: 0.00069 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 2200 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 2300 | Loss: 0.00048 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 2400 | Loss: 0.00271 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 010 | Batch: 2500 | Loss: 0.00563 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 000 | Loss: 0.05023 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 100 | Loss: 0.00161 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 200 | Loss: 0.00475 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 300 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 400 | Loss: 0.00107 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 500 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 600 | Loss: 0.00645 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 700 | Loss: 0.00046 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 800 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 900 | Loss: 0.00034 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1000 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1100 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1200 | Loss: 0.00018 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1300 | Loss: 0.00357 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1400 | Loss: 0.00054 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1500 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1600 | Loss: 0.00222 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1700 | Loss: 0.00036 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1800 | Loss: 0.00031 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 1900 | Loss: 0.01442 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 2000 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 2100 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 2200 | Loss: 0.00034 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 2300 | Loss: 0.00048 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 2400 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 011 | Batch: 2500 | Loss: 0.00040 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 000 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 100 | Loss: 0.00531 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 200 | Loss: 0.00250 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 300 | Loss: 0.00030 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 400 | Loss: 0.00111 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 500 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 600 | Loss: 0.03621 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 700 | Loss: 0.00235 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 800 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 900 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1000 | Loss: 0.00049 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1100 | Loss: 0.00513 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1200 | Loss: 0.00054 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1400 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1500 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1600 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1700 | Loss: 0.00381 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1800 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 1900 | Loss: 0.01697 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 2000 | Loss: 0.00330 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 2100 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 2300 | Loss: 0.13605 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 2400 | Loss: 0.00608 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 011 | Batch: 2500 | Loss: 0.00060 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 000 | Loss: 0.00997 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 100 | Loss: 0.00461 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 200 | Loss: 0.00047 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 300 | Loss: 0.00274 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 400 | Loss: 0.00067 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 500 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 700 | Loss: 0.03191 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 800 | Loss: 0.00982 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 900 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1000 | Loss: 0.00309 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1100 | Loss: 0.00056 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1200 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1300 | Loss: 0.05669 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1400 | Loss: 0.00037 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1500 | Loss: 0.00151 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1600 | Loss: 0.08335 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1700 | Loss: 0.00176 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1800 | Loss: 0.00156 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 1900 | Loss: 0.00025 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 2000 | Loss: 0.00578 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 2100 | Loss: 0.00281 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 2200 | Loss: 0.00052 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 2300 | Loss: 0.00579 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 2400 | Loss: 0.04475 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 011 | Batch: 2500 | Loss: 0.00850 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 000 | Loss: 0.02064 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 100 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 200 | Loss: 0.00112 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 300 | Loss: 0.05354 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 400 | Loss: 0.00024 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 500 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 600 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 700 | Loss: 0.01737 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 900 | Loss: 0.01446 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1000 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1100 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1400 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1500 | Loss: 0.00461 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1600 | Loss: 0.00810 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1700 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1800 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 1900 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 2000 | Loss: 0.07625 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 2100 | Loss: 0.00419 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 2200 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 2300 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 2400 | Loss: 0.00109 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 011 | Batch: 2500 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 000 | Loss: 0.00025 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 100 | Loss: 0.00122 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 200 | Loss: 0.00225 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 300 | Loss: 0.07512 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 400 | Loss: 0.00164 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 500 | Loss: 0.00033 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 600 | Loss: 0.00166 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 700 | Loss: 0.00108 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 900 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1000 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1300 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1400 | Loss: 0.00478 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1500 | Loss: 0.00170 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1600 | Loss: 0.00156 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1700 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1800 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 2000 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 2200 | Loss: 0.00487 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 2300 | Loss: 0.00056 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 2400 | Loss: 0.00036 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 012 | Batch: 2500 | Loss: 0.00035 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 000 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 100 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 200 | Loss: 0.00199 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 300 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 400 | Loss: 0.00056 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 500 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 600 | Loss: 0.00181 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 700 | Loss: 0.00051 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 800 | Loss: 0.00030 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 900 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1000 | Loss: 0.00062 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1200 | Loss: 0.01492 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1300 | Loss: 0.12673 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1400 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1500 | Loss: 0.00499 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1600 | Loss: 0.00294 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1700 | Loss: 0.00018 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1800 | Loss: 0.00091 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 1900 | Loss: 0.06090 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 2000 | Loss: 0.00366 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 2100 | Loss: 0.00050 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 2200 | Loss: 0.00057 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 2300 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 2400 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 012 | Batch: 2500 | Loss: 0.00029 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 000 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 100 | Loss: 0.00349 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 200 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 300 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 400 | Loss: 0.00361 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 500 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 600 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 700 | Loss: 0.00077 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 800 | Loss: 0.00038 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 900 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1100 | Loss: 0.00093 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1200 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1300 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1400 | Loss: 0.00303 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1500 | Loss: 0.00370 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1600 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1700 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1800 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 1900 | Loss: 0.11456 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 2000 | Loss: 0.00024 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 2100 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 2200 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 2300 | Loss: 0.03481 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 2400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 012 | Batch: 2500 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 000 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 100 | Loss: 0.00078 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 300 | Loss: 0.00329 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 400 | Loss: 0.00175 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 500 | Loss: 0.00135 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 600 | Loss: 0.28609 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 700 | Loss: 0.00181 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 800 | Loss: 0.00119 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 900 | Loss: 0.02143 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1000 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1100 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1200 | Loss: 0.00047 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1300 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1400 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1500 | Loss: 0.00232 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1600 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1700 | Loss: 0.00102 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1800 | Loss: 0.00529 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 1900 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 2000 | Loss: 0.00776 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 2100 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 2300 | Loss: 0.00477 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 2400 | Loss: 0.04301 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 012 | Batch: 2500 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 000 | Loss: 0.00031 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 100 | Loss: 0.00019 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 200 | Loss: 0.00057 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 300 | Loss: 0.00070 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 400 | Loss: 0.01801 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 500 | Loss: 0.00055 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 600 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 700 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1000 | Loss: 0.00169 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1100 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1200 | Loss: 0.00145 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1300 | Loss: 0.00068 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1400 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1500 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1600 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1700 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 2000 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 2100 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 2200 | Loss: 0.00249 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 2400 | Loss: 0.00093 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 013 | Batch: 2500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 000 | Loss: 0.00061 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 100 | Loss: 0.00875 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 200 | Loss: 0.00143 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 300 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 400 | Loss: 0.00054 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 500 | Loss: 0.02339 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 600 | Loss: 0.09524 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 700 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 900 | Loss: 0.00079 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1000 | Loss: 0.00257 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1100 | Loss: 0.00211 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1200 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1300 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1400 | Loss: 0.00037 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1500 | Loss: 0.00569 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1600 | Loss: 0.00038 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1700 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1800 | Loss: 0.00257 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 1900 | Loss: 0.00023 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 2000 | Loss: 0.00137 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 2100 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 2300 | Loss: 0.01822 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 2400 | Loss: 0.00031 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 013 | Batch: 2500 | Loss: 0.02788 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 000 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 100 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 200 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 300 | Loss: 0.00059 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 500 | Loss: 0.00305 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 600 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 700 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 800 | Loss: 0.01421 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 900 | Loss: 0.00041 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1000 | Loss: 0.00036 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1200 | Loss: 0.00414 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1300 | Loss: 0.00186 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1400 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1500 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1700 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1800 | Loss: 0.00165 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 2000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 2100 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 2200 | Loss: 0.07948 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 2300 | Loss: 0.00270 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 2400 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 013 | Batch: 2500 | Loss: 0.00036 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 000 | Loss: 0.00120 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 100 | Loss: 0.00558 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 200 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 300 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 400 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 700 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 800 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 900 | Loss: 0.00031 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1000 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1100 | Loss: 0.00293 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1200 | Loss: 0.00051 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1300 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1500 | Loss: 0.00577 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1600 | Loss: 0.00272 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1700 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 1900 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 2000 | Loss: 0.00087 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 2100 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 2200 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 2300 | Loss: 0.02183 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 2400 | Loss: 0.00085 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 013 | Batch: 2500 | Loss: 0.00031 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 000 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 100 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 200 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 300 | Loss: 0.04107 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 500 | Loss: 0.00672 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 600 | Loss: 0.00110 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 800 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 900 | Loss: 0.08828 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1000 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1100 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1200 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1300 | Loss: 0.00029 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1400 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1500 | Loss: 0.00158 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1600 | Loss: 0.00037 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1800 | Loss: 0.00043 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 1900 | Loss: 0.03030 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 2200 | Loss: 0.00521 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 2400 | Loss: 0.05922 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 014 | Batch: 2500 | Loss: 0.00128 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 000 | Loss: 0.01265 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 100 | Loss: 0.00048 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 200 | Loss: 0.04682 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 300 | Loss: 0.00018 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 400 | Loss: 0.00075 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 500 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 600 | Loss: 0.00025 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 900 | Loss: 0.00039 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1000 | Loss: 0.00053 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1200 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1300 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1500 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1600 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1700 | Loss: 0.00046 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1800 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 1900 | Loss: 0.00572 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 2000 | Loss: 0.01094 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 2100 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 2400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 014 | Batch: 2500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 000 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 100 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 200 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 300 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 500 | Loss: 0.13766 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 600 | Loss: 0.00472 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 700 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 800 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 900 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1000 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1100 | Loss: 0.00067 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1200 | Loss: 0.00058 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1400 | Loss: 0.00029 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1500 | Loss: 0.00038 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1600 | Loss: 0.00133 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1700 | Loss: 0.07545 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 1900 | Loss: 0.00059 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 2000 | Loss: 0.01054 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 2100 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 2200 | Loss: 0.00148 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 2300 | Loss: 0.00019 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 2400 | Loss: 0.00849 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 014 | Batch: 2500 | Loss: 0.00027 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 000 | Loss: 0.01648 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 100 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 200 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 300 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 400 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 500 | Loss: 0.00237 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 600 | Loss: 0.00356 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 800 | Loss: 0.00099 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 900 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1000 | Loss: 0.00493 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1100 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1500 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1700 | Loss: 0.00307 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1800 | Loss: 0.03932 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 1900 | Loss: 0.00050 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 2000 | Loss: 0.00209 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 2100 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 2200 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 2300 | Loss: 0.00271 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 2400 | Loss: 0.00066 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 014 | Batch: 2500 | Loss: 0.01733 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 000 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 100 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 600 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 800 | Loss: 0.00165 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 900 | Loss: 0.00035 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1000 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1200 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1300 | Loss: 0.01847 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1400 | Loss: 0.00082 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1500 | Loss: 0.00211 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1600 | Loss: 0.00085 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1700 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1800 | Loss: 0.00079 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 1900 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 2100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 2200 | Loss: 0.01035 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 2300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 2400 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 015 | Batch: 2500 | Loss: 0.00137 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 000 | Loss: 0.01824 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 100 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 200 | Loss: 0.00038 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 400 | Loss: 0.00116 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 500 | Loss: 0.00078 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 900 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1100 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1200 | Loss: 0.00125 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1300 | Loss: 0.00085 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1400 | Loss: 0.00032 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1500 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 1900 | Loss: 0.00026 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 2000 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 2100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 2200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 2300 | Loss: 0.21449 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 2400 | Loss: 0.00051 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 015 | Batch: 2500 | Loss: 0.00027 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 100 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 200 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 300 | Loss: 0.00050 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 400 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 500 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 600 | Loss: 0.00880 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 700 | Loss: 0.00288 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 800 | Loss: 0.00133 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1100 | Loss: 0.00245 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1200 | Loss: 0.00170 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1300 | Loss: 0.00054 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1400 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1600 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1700 | Loss: 0.00603 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1800 | Loss: 0.13276 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 1900 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 2000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 2300 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 2400 | Loss: 0.00039 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 015 | Batch: 2500 | Loss: 0.00033 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 000 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 100 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 200 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 400 | Loss: 0.00029 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 500 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 700 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1200 | Loss: 0.00103 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1300 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1500 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1700 | Loss: 0.19879 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1800 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 1900 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 2000 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 2100 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 2200 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 2300 | Loss: 0.01266 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 2400 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 015 | Batch: 2500 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 000 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 100 | Loss: 0.00019 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 200 | Loss: 0.02205 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 500 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 600 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 700 | Loss: 0.00051 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 800 | Loss: 0.00018 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 900 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1200 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1300 | Loss: 0.00440 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1500 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1800 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 1900 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 2000 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 2100 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 2300 | Loss: 0.00027 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 016 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 000 | Loss: 0.00034 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 200 | Loss: 0.00023 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 600 | Loss: 0.00156 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 700 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 800 | Loss: 0.00033 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 900 | Loss: 0.00036 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1000 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1200 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1500 | Loss: 0.00102 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1600 | Loss: 0.00026 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1700 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1800 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 1900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 2000 | Loss: 0.00039 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 2100 | Loss: 0.05321 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 2200 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 2300 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 2400 | Loss: 0.00094 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 016 | Batch: 2500 | Loss: 0.00029 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 000 | Loss: 0.00034 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 100 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 200 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 300 | Loss: 0.00026 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 400 | Loss: 0.00138 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 600 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 700 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 900 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1100 | Loss: 0.00037 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1200 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1600 | Loss: 0.00244 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1700 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 1900 | Loss: 0.00039 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 2000 | Loss: 0.03756 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 2100 | Loss: 0.00029 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 2300 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 2400 | Loss: 0.00046 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 016 | Batch: 2500 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 000 | Loss: 0.00041 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 200 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 400 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 500 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 700 | Loss: 0.00474 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 800 | Loss: 0.07070 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1000 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1100 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1200 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1700 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1800 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 1900 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 2000 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 2100 | Loss: 0.00245 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 2200 | Loss: 0.00926 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 2300 | Loss: 0.00092 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 2400 | Loss: 0.00809 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 016 | Batch: 2500 | Loss: 0.01604 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 000 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 100 | Loss: 0.00120 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 200 | Loss: 0.02224 | Correct: 31/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 300 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 400 | Loss: 0.00112 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 600 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 700 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 900 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1000 | Loss: 0.00038 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1100 | Loss: 0.00057 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1200 | Loss: 0.00296 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1400 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1600 | Loss: 0.00089 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1700 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 1900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 2300 | Loss: 0.00030 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 2400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 017 | Batch: 2500 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 000 | Loss: 0.00265 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 100 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 200 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 400 | Loss: 0.00141 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 600 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 800 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 900 | Loss: 0.00511 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1000 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1100 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1200 | Loss: 0.00068 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1300 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 1900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 2100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 2200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 2300 | Loss: 0.00209 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 2400 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 017 | Batch: 2500 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 500 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 600 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 800 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 900 | Loss: 0.00026 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1000 | Loss: 0.05751 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1100 | Loss: 0.02006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1200 | Loss: 0.00134 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1300 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1400 | Loss: 0.00062 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1500 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1700 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 2000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 2100 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 2300 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 2400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 017 | Batch: 2500 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 100 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 200 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 300 | Loss: 0.00061 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 400 | Loss: 0.00149 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 700 | Loss: 0.00036 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 800 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 900 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1000 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1100 | Loss: 0.00064 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1300 | Loss: 0.00026 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1500 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1600 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1800 | Loss: 0.00041 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 1900 | Loss: 0.00122 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 2100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 2300 | Loss: 0.02041 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 2400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 017 | Batch: 2500 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 000 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 300 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 600 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 700 | Loss: 0.00028 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1000 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1100 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1700 | Loss: 0.00333 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1800 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 1900 | Loss: 0.01752 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 2000 | Loss: 0.00063 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 2100 | Loss: 0.00018 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 2200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 2300 | Loss: 0.00088 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 2400 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 018 | Batch: 2500 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 100 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 500 | Loss: 0.00157 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 800 | Loss: 0.00019 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 900 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1200 | Loss: 0.00039 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1300 | Loss: 0.00016 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 1900 | Loss: 0.00086 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 2000 | Loss: 0.00027 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 2100 | Loss: 0.06604 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 2200 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 018 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 500 | Loss: 0.00092 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 700 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 800 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1100 | Loss: 0.00083 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1500 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1800 | Loss: 0.00440 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 2000 | Loss: 0.00609 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 2200 | Loss: 0.00128 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 2300 | Loss: 0.02269 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 018 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 300 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 400 | Loss: 0.00035 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 800 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 900 | Loss: 0.00033 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1100 | Loss: 0.08607 | Correct: 31/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1200 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1300 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1500 | Loss: 0.00035 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1600 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 2100 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 2300 | Loss: 0.00170 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 2400 | Loss: 0.00073 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 018 | Batch: 2500 | Loss: 0.00095 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 000 | Loss: 0.00019 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 100 | Loss: 0.00265 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 700 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 800 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 900 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1600 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1700 | Loss: 0.00096 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 019 | Batch: 2500 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 200 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 900 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1000 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1100 | Loss: 0.00147 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1200 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1500 | Loss: 0.00056 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1700 | Loss: 0.00030 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1800 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 2000 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 2100 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 2400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 019 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 200 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 300 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 400 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 700 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 800 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1200 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1800 | Loss: 0.04470 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 1900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 2000 | Loss: 0.00088 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 2300 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 2400 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 019 | Batch: 2500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 000 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 500 | Loss: 0.00190 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 1900 | Loss: 0.00124 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 2300 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 019 | Batch: 2500 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 400 | Loss: 0.00042 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 500 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1000 | Loss: 0.00024 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1400 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1500 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 1900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 2000 | Loss: 0.00033 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 2100 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 2200 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 2300 | Loss: 0.00038 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 2400 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 020 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 400 | Loss: 0.02323 | Correct: 31/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 500 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1500 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 1900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 2000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 2100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 2400 | Loss: 0.00045 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 020 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 100 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 200 | Loss: 0.00079 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 300 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 600 | Loss: 0.00457 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 1900 | Loss: 0.00178 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 2000 | Loss: 0.02165 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 2300 | Loss: 0.00049 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 020 | Batch: 2500 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 200 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 500 | Loss: 0.00034 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 700 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 900 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1000 | Loss: 0.00365 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1100 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1200 | Loss: 0.00055 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1400 | Loss: 0.00259 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1500 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1600 | Loss: 0.00094 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 1900 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 2200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 2300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 020 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1100 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1400 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1500 | Loss: 0.00022 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 2200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 2300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 2400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 021 | Batch: 2500 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1700 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 2000 | Loss: 0.00064 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 2400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 021 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 100 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 200 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 400 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1400 | Loss: 0.00048 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1700 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 2000 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 021 | Batch: 2500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 500 | Loss: 0.00015 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 700 | Loss: 0.00017 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 900 | Loss: 0.00043 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1000 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1400 | Loss: 0.00058 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1700 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1800 | Loss: 0.00046 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 2000 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 2100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 2400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 021 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 100 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1200 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1700 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 2000 | Loss: 0.00033 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 022 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 300 | Loss: 0.00462 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 800 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1200 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 2000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 2100 | Loss: 0.00011 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 2300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 2400 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 022 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 800 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1000 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1800 | Loss: 0.02143 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 2100 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 022 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 200 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 600 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1100 | Loss: 0.00014 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1700 | Loss: 0.00242 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1800 | Loss: 0.00048 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 1900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 022 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 400 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 600 | Loss: 0.00020 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 800 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 1900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 2400 | Loss: 0.00019 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 023 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1200 | Loss: 0.00040 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1500 | Loss: 0.00106 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 1900 | Loss: 0.00023 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 2000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 2300 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 023 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 1900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 023 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 300 | Loss: 0.00034 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 2200 | Loss: 0.00021 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 2400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 023 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 400 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1700 | Loss: 0.00024 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 1900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 2000 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 2300 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 024 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 400 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1500 | Loss: 0.00013 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 1900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 2300 | Loss: 0.00074 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 024 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 100 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 600 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 700 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 1900 | Loss: 0.02696 | Correct: 31/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 024 | Batch: 2500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 300 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 800 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 1900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 2200 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 024 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 700 | Loss: 0.00007 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 1900 | Loss: 0.00010 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 025 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1200 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1400 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 2000 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 025 | Batch: 2500 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1000 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1400 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 2300 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 025 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 900 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1400 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1600 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 025 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 100 | Loss: 0.00008 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 300 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 700 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 800 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 026 | Batch: 2500 | Loss: 0.00006 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 000 | Loss: 0.00012 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 500 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1500 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1800 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 2000 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 026 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1800 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 1900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 2200 | Loss: 0.00034 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 026 | Batch: 2500 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 1900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 026 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1000 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 1900 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 027 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 000 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 027 | Batch: 2500 | Loss: 0.00004 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1100 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 027 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 000 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 2300 | Loss: 0.00002 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 027 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 028 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 2400 | Loss: 0.00005 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 028 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1600 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 2300 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 028 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 400 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1000 | Loss: 0.00054 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 2000 | Loss: 0.01098 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 028 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 2200 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 000 | Epoch: 029 | Batch: 2500 | Loss: 0.00003 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1200 | Loss: 0.00156 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1400 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1700 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 001 | Epoch: 029 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 900 | Loss: 0.00001 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 002 | Epoch: 029 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 500 | Loss: 0.00009 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1500 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1600 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1700 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1800 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 1900 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 2000 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 2100 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 2200 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 2300 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 2400 | Loss: 0.00000 | Correct: 32/32\n",
      "Estimator: 003 | Epoch: 029 | Batch: 2500 | Loss: 0.00000 | Correct: 32/32\n"
     ]
    }
   ],
   "source": [
    "!pip install torchensemble\n",
    "from torchensemble import VotingClassifier\n",
    "from torchvision import models\n",
    "new_classifier = models.resnet18(pretrained = False, num_classes=10).cuda()\n",
    "new_classifier.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=False).cuda()\n",
    "\n",
    "model = VotingClassifier(\n",
    "    estimator=new_classifier,\n",
    "    n_estimators=4,\n",
    "    cuda=True,\n",
    ")\n",
    "\n",
    "model.set_optimizer(\"Adam\", lr=0.001)\n",
    "model.set_scheduler(\n",
    "    \"CosineAnnealingLR\",\n",
    "    T_max=30,\n",
    ")\n",
    "\n",
    "model.fit(combine_dataloader, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2336ea44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T17:18:14.558754Z",
     "iopub.status.busy": "2023-05-25T17:18:14.558383Z",
     "iopub.status.idle": "2023-05-25T17:19:49.819031Z",
     "shell.execute_reply": "2023-05-25T17:19:49.818003Z"
    },
    "papermill": {
     "duration": 96.579369,
     "end_time": "2023-05-25T17:19:49.821693",
     "exception": false,
     "start_time": "2023-05-25T17:18:13.242324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "# label_predictor.eval()\n",
    "# feature_extractor.eval()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (test_data, _) in enumerate(test_dataloader):\n",
    "        \n",
    "        test_data = test_data.cuda()\n",
    "        # class_logits = label_predictor(feature_extractor(test_data))\n",
    "\n",
    "        # x = torch.argmax(class_logits, dim=1).cpu().detach().numpy()\n",
    "        outputs = model.predict(test_data)\n",
    "        x = torch.argmax(outputs, 1).cpu().detach().numpy()\n",
    "        result.append(x)\n",
    "\n",
    "import pandas as pd\n",
    "result = np.concatenate(result)\n",
    "\n",
    "# Generate your submission\n",
    "df = pd.DataFrame({'id': np.arange(0,len(result)), 'label': result})\n",
    "df.to_csv('DaNN_submission_ensem_2.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f546a3d0",
   "metadata": {
    "id": "CpheoH_rvFbO",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Training Statistics\n",
    "\n",
    "- Number of parameters:\n",
    "  - Feature Extractor: 2, 142, 336\n",
    "  - Label Predictor: 530, 442\n",
    "  - Domain Classifier: 1, 055, 233\n",
    "\n",
    "- Simple\n",
    " - Training time on colab: ~ 1 hr\n",
    "- Medium\n",
    " - Training time on colab: 2 ~ 4 hr\n",
    "- Strong\n",
    " - Training time on colab: 5 ~ 6 hrs\n",
    "- Boss\n",
    " - **Unmeasurable**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14515e0c",
   "metadata": {
    "id": "GYO8InxavGsy",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Learning Curve (Strong Baseline)\n",
    "* This method is slightly different from colab.\n",
    "\n",
    "![Loss Curve](https://i.imgur.com/vIujQyo.png)\n",
    "\n",
    "# Accuracy Curve (Strong Baseline)\n",
    "* Note that you cannot access testing accuracy. But this plot tells you that even though the model overfits the training data, the testing accuracy is still improving, and that's why you need to train more epochs.\n",
    "\n",
    "![Acc Curve](https://i.imgur.com/4W1otXG.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bef9865",
   "metadata": {
    "id": "s6UfXzef-wNl",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Q&A\n",
    "\n",
    "If there is any problem related to Domain Adaptation, please email to b08902047@ntu.edu.tw / mlta-2023-spring@googlegroups.com。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ec025",
   "metadata": {
    "id": "C4TMXG_YCqVb",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8487.685198,
   "end_time": "2023-05-25T17:19:56.966407",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-25T14:58:29.281209",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

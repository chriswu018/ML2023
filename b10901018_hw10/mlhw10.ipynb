{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Homework 10 - Adversarial Attack**","metadata":{"id":"H9m2AbpHC9vS"}},{"cell_type":"markdown","source":"## Enviroment & Download\n\nWe make use of [pytorchcv](https://pypi.org/project/pytorchcv/) to obtain CIFAR-10 pretrained model, so we need to set up the enviroment first. We also need to download the data (200 images) which we want to attack.","metadata":{"id":"k0G8g5KuDBzU"}},{"cell_type":"code","source":"# set up environment\n!pip install pytorchcv\n!pip install imgaug","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMK1RhUQCz1e","outputId":"00ccab9c-0b4d-4c9a-d3f1-8b6c542e61c6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download\n#!gdown --id 1t2UFQXr1cr5qLMBK2oN2rY1NDypi9Nyw --output data.zip\n\n# if the above link isn't available, try this one\n!wget https://www.dropbox.com/s/lbpypqamqjpt2qz/data.zip\n\n# unzip\n!unzip ./data.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm ./data.zip","metadata":{"id":"-a6naDouEWUZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom pytorchcv.model_provider import get_model as ptcv_get_model\nimport random\nimport numpy as np\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 8\n\ndef same_seeds(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\nsame_seeds(0) ","metadata":{"id":"SaEEx0Y3DMdu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Global Settings \n#### **[NOTE]**: Don't change the settings here, or your generated image might not meet the constraint.\n* $\\epsilon$ is fixed to be 8. But on **Data section**, we will first apply transforms on raw pixel value (0-255 scale) **by ToTensor (to 0-1 scale)** and then **Normalize (subtract mean divide std)**. $\\epsilon$ should be set to $\\frac{8}{255 * std}$ during attack.\n\n* Explaination (optional)\n    * Denote the first pixel of original image as $p$, and the first pixel of adversarial image as $a$.\n    * The $\\epsilon$ constraints tell us $\\left| p-a \\right| <= 8$.\n    * ToTensor() can be seen as a function where $T(x) = x/255$.\n    * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n    * After applying ToTensor() and Normalize() on $p$ and $a$, the constraint becomes $\\left| N(T(p))-N(T(a)) \\right| = \\left| \\frac{\\frac{p}{255}-mean}{std}-\\frac{\\frac{a}{255}-mean}{std} \\right| = \\frac{1}{255 * std} \\left| p-a \\right| <= \\frac{8}{255 * std}.$\n    * So, we should set $\\epsilon$ to be $\\frac{8}{255 * std}$ after ToTensor() and Normalize().","metadata":{"id":"Z8mIr7c0DPsh"}},{"cell_type":"code","source":"# the mean and std are the calculated statistics from cifar_10 dataset\ncifar_10_mean = (0.491, 0.482, 0.447) # mean for the three channels of cifar_10 images\ncifar_10_std = (0.202, 0.199, 0.201) # std for the three channels of cifar_10 images\n\n# convert mean and std to 3-dimensional tensors for future operations\nmean = torch.tensor(cifar_10_mean).to(device).view(3, 1, 1)\nstd = torch.tensor(cifar_10_std).to(device).view(3, 1, 1)\n\nepsilon = 8/255/std","metadata":{"id":"IBdYgS2DDNL5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root = './data' # directory for storing benign images\n# benign images: images which do not contain adversarial perturbations\n# adversarial images: images which include adversarial perturbations","metadata":{"id":"AjNkQLoaDWba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data\n\nConstruct dataset and dataloader from root directory. Note that we store the filename of each image for future usage.","metadata":{"id":"sNf-LoODDZXB"}},{"cell_type":"code","source":"import os\nimport glob\nimport shutil\nimport numpy as np\nfrom PIL import Image\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(cifar_10_mean, cifar_10_std)\n])\n\nclass AdvDataset(Dataset):\n    def __init__(self, data_dir, transform):\n        self.images = []\n        self.labels = []\n        self.names = []\n        '''\n        data_dir\n        ├── class_dir\n        │   ├── class1.png\n        │   ├── ...\n        │   ├── class20.png\n        '''\n        for i, class_dir in enumerate(sorted(glob.glob(f'{data_dir}/*'))):\n            images = sorted(glob.glob(f'{class_dir}/*'))\n            self.images += images\n            self.labels += ([i] * len(images))\n            self.names += [os.path.relpath(imgs, data_dir) for imgs in images]\n        self.transform = transform\n    def __getitem__(self, idx):\n        image = self.transform(Image.open(self.images[idx]))\n        label = self.labels[idx]\n        return image, label\n    def __getname__(self):\n        return self.names\n    def __len__(self):\n        return len(self.images)\n\nadv_set = AdvDataset(root, transform=transform)\nadv_names = adv_set.__getname__()\nadv_loader = DataLoader(adv_set, batch_size=batch_size, shuffle=False)\n\nprint(f'number of images = {adv_set.__len__()}')","metadata":{"id":"lV7rbnD5DarR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3610d736-4ce6-4054-9a9e-28cbe2cfad95","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils -- Benign Images Evaluation","metadata":{"id":"C9D7eakEDflF"}},{"cell_type":"code","source":"# to evaluate the performance of model on benign images\ndef epoch_benign(model, loader, loss_fn):\n    model.eval()\n    train_acc, train_loss = 0.0, 0.0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        yp = model(x)\n        loss = loss_fn(yp, y)\n        train_acc += (yp.argmax(dim=1) == y).sum().item()\n        train_loss += loss.item() * x.shape[0]\n    return train_acc / len(loader.dataset), train_loss / len(loader.dataset)","metadata":{"id":"byE4VH3uDduA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils -- Attack Algorithm","metadata":{"id":"D3L_qtufDk4j"}},{"cell_type":"code","source":"# alpha and num_iter can be decided by yourself\nalpha = 0.8/255/std\n\n#REF: https://github.com/Harry24k/adversarial-attacks-pytorch/blob/master/torchattacks/attacks/difgsm.py\n\ndef input_diversity(x, resize_rate, diversity_prob):\n    img_size = x.shape[-1]\n    img_resize = int(img_size * resize_rate)\n\n    if resize_rate < 1:\n        img_size = img_resize\n        img_resize = x.shape[-1]\n\n    rnd = torch.randint(low=img_size, high=img_resize, size=(1,), dtype=torch.int32)\n    rescaled = nn.functional.interpolate(x, size=[rnd, rnd], mode='bilinear', align_corners=False)\n    h_rem = img_resize - rnd\n    w_rem = img_resize - rnd\n    pad_top = torch.randint(low=0, high=h_rem.item(), size=(1,), dtype=torch.int32)\n    pad_bottom = h_rem - pad_top\n    pad_left = torch.randint(low=0, high=w_rem.item(), size=(1,), dtype=torch.int32)\n    pad_right = w_rem - pad_left\n\n    padded = nn.functional.pad(rescaled, [pad_left.item(), pad_right.item(), pad_top.item(), pad_bottom.item()], value=0)\n\n    return padded if torch.rand(1) < diversity_prob else x\n\n# num_iter=20 -> 40\ndef dmifgsm(model, x, y, loss_fn, epsilon=epsilon, alpha=alpha, num_iter=40, decay=1.0,\n            resize_rate=0.9, diversity_prob=0.5):\n    x_adv = x\n    # initialze momentum tensor\n    momentum = torch.zeros_like(x).detach().to(device)\n\n    # write a loop of num_iter to represent the iterative times\n    for i in range(num_iter):\n        x_adv = x_adv.detach().clone()\n        x_adv.requires_grad = True # need to obtain gradient of x_adv, thus set required grad\n        loss = loss_fn(model(input_diversity(x_adv, resize_rate, diversity_prob)), y) # calculate loss\n        loss.backward() # calculate gradient\n        # TODO: Momentum calculation\n        # Update adversarial images\n        # grad = .....\n        # grad = torch.autograd.grad(loss, x_adv, retain_graph=False, create_graph=False)[0]\n        grad = x_adv.grad.detach()\n        grad = grad / torch.mean(torch.abs(grad), dim=(1,2,3), keepdim=True)\n        grad = grad + momentum * decay\n        momentum = grad\n        x_adv = x_adv + alpha * grad.sign()\n        x_adv = torch.max(torch.min(x_adv, x+epsilon), x-epsilon) # clip new x_adv back to [x-epsilon, x+epsilon]\n    return x_adv","metadata":{"id":"odTOhtrtDklT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils -- Attack\n* Recall\n  * ToTensor() can be seen as a function where $T(x) = x/255$.\n  * Normalize() can be seen as a function where $N(x) = (x-mean)/std$ where $mean$ and $std$ are constants.\n\n* Inverse function\n  * Inverse Normalize() can be seen as a function where $N^{-1}(x) = x*std+mean$ where $mean$ and $std$ are constants.\n  * Inverse ToTensor() can be seen as a function where $T^{-1}(x) = x*255$.\n\n* Special Noted\n  * ToTensor() will also convert the image from shape (height, width, channel) to shape (channel, height, width), so we also need to transpose the shape back to original shape.\n  * Since our dataloader samples a batch of data, what we need here is to transpose **(batch_size, channel, height, width)** back to **(batch_size, height, width, channel)** using np.transpose.","metadata":{"id":"0o9ww4s1DrEx"}},{"cell_type":"code","source":"# perform adversarial attack and generate adversarial examples\ndef gen_adv_examples(model, loader, attack, loss_fn):\n    model.eval()\n    adv_names = []\n    train_acc, train_loss = 0.0, 0.0\n    for i, (x, y) in enumerate(loader):\n        x, y = x.to(device), y.to(device)\n        x_adv = attack(model, x, y, loss_fn) # obtain adversarial examples\n        yp = model(x_adv)\n        loss = loss_fn(yp, y)\n        train_acc += (yp.argmax(dim=1) == y).sum().item()\n        train_loss += loss.item() * x.shape[0]\n        # store adversarial examples\n        adv_ex = ((x_adv) * std + mean).clamp(0, 1) # to 0-1 scale\n        adv_ex = (adv_ex * 255).clamp(0, 255) # 0-255 scale\n        adv_ex = adv_ex.detach().cpu().data.numpy().round() # round to remove decimal part\n        adv_ex = adv_ex.transpose((0, 2, 3, 1)) # transpose (bs, C, H, W) back to (bs, H, W, C)\n        adv_examples = adv_ex if i == 0 else np.r_[adv_examples, adv_ex]\n    return adv_examples, train_acc / len(loader.dataset), train_loss / len(loader.dataset)\n\n# create directory which stores adversarial examples\ndef create_dir(data_dir, adv_dir, adv_examples, adv_names):\n    if os.path.exists(adv_dir) is not True:\n        _ = shutil.copytree(data_dir, adv_dir)\n    for example, name in zip(adv_examples, adv_names):\n        im = Image.fromarray(example.astype(np.uint8)) # image pixel value should be unsigned int\n        im.save(os.path.join(adv_dir, name))","metadata":{"id":"rbtfv7rjDrvR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model / Loss Function\n\nModel list is available [here](https://github.com/osmr/imgclsmob/blob/master/pytorch/pytorchcv/model_provider.py). Please select models which has _cifar10 suffix. Other kinds of models are prohibited, and it will be considered to be cheating if you use them. \n\nNote: Some of the models cannot be accessed/loaded. You can safely skip them since TA's model will not use those kinds of models.","metadata":{"id":"rbLBR4bjDu7h"}},{"cell_type":"code","source":"# This function is used to check whether you use models pretrained on cifar10 instead of other datasets\ndef model_checker(model_name):\n    assert ('cifar10' in model_name) and ('cifar100' not in model_name), 'The model selected is not pretrained on cifar10!'","metadata":{"id":"xCKMshb08I1I","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorchcv.model_provider import get_model as ptcv_get_model","metadata":{"id":"eCJU4k__DwPT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f42b7d3-2d0a-4ba3-c29b-17242b8b62de","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Example of Ensemble Attack\n* Ensemble multiple models as your proxy model to increase the black-box transferability ([paper](https://arxiv.org/abs/1611.02770))","metadata":{"id":"7dq5LDvJD5rB"}},{"cell_type":"markdown","source":"* Construct your ensemble model","metadata":{"id":"De5J9n3WD-56"}},{"cell_type":"code","source":"#from tqdm.auto import tqdm\ndef train_ep(model, train_loader, lr=1e-3):\n    for epoch in range(5):\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 5, eta_min=lr*1e-2)\n        model.train()\n        for batch in (train_loader):\n            imgs, labels = batch\n            logits = model(imgs.to(device))\n\n            loss = criterion(logits, labels.to(device))\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n            optimizer.step()\n            scheduler.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bad_model(name):\n    \n    benign_acc = 0\n    benign_loss = 5\n    loss_fn = nn.CrossEntropyLoss()\n    ct = 0\n    \n    model0 = ptcv_get_model(name, pretrained=True).to(device)\n    model2 = ptcv_get_model(name, pretrained=False).to(device)\n    lr = 1e-5\n    if name[:3] == 'ser':\n        #model0.features.stage1.unit1.se = model2.features.stage1.unit1.se\n        lr = 7e-4\n    elif name[:3] == 'sep':\n        lr = 5e-4\n    elif name[:4] == 'diap':\n        lr = 3e-4\n    elif name[:4] == 'diar':\n        model0.features.stage1.unit1.attention = model2.features.stage1.unit1.attention\n    elif name == 'resnet20_cifar10' or name[1:3] == 're' :\n        model0.features.stage1.unit1.body.conv1.conv = model2.features.stage1.unit1.body.conv1.conv\n    #else:\n    #    benign_acc, benign_loss = epoch_benign(model0, adv_loader, loss_fn)\n    #    print(f'model:{name}, benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')\n    #    return model0\n    \n    while(benign_acc<0.8):\n        train_ep(model0, adv_loader, lr)\n        benign_acc, benign_loss = epoch_benign(model0, adv_loader, loss_fn)\n        ct+=1\n    print(5*ct)\n    print(f'model:{name}, benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')\n    \n    return model0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################ BOSS BASELINE ######################\n\nclass ensembleNet(nn.Module):\n    def __init__(self, model_names):\n        super().__init__()\n        self.models = nn.ModuleList([ptcv_get_model(name, pretrained=True) for name in model_names])\n        #self.models = nn.ModuleList([bad_model(name) for name in model_names])\n    def forward(self, x):\n        ensemble_logits = 0\n        for i, m in enumerate(self.models):\n            # TODO: sum up logits from multiple models  \n            # return ensemble_logits\n            logits = m(x.clone())\n            ensemble_logits += logits\n        ensemble_logits = ensemble_logits / len(self.models)\n        return ensemble_logits","metadata":{"id":"nvEX_IM8D7Rx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names = [\n    'resnet20_cifar10', \n    'preresnet20_cifar10', \n    'seresnet20_cifar10',  \n    'sepreresnet20_cifar10',\n    'diaresnet20_cifar10',\n    'diapreresnet20_cifar10', \n    'resnext29_16x64d_cifar10',\n    'pyramidnet110_a48_cifar10',\n    'densenet40_k12_cifar10',\n    'wrn16_10_cifar10',\n    'shakeshakeresnet20_2x16d_cifar10',\n]\n\nfor model_name in model_names:\n    model_checker(model_name)\n\nensemble_model = ensembleNet(model_names).to(device)\npp = ensemble_model.eval()","metadata":{"id":"9as1WHEiD_cp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6ec6f50-da64-437b-d793-ac87d0baddfd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\n\nbenign_acc, benign_loss = epoch_benign(ensemble_model, adv_loader, loss_fn)\nprint(f'benign_acc = {benign_acc:.5f}, benign_loss = {benign_loss:.5f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adv_examples, ifgsm_acc, ifgsm_loss = gen_adv_examples(ensemble_model, adv_loader, dmifgsm, loss_fn)\nprint(f'dmifgsm_acc = {ifgsm_acc:.5f}, dmifgsm_loss = {ifgsm_loss:.5f}')\n\ncreate_dir(root, \"dmifgsm\", adv_examples, adv_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/dmifgsm\n!tar zcvf ../dmifgsm.tgz *\n%cd ..","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{"id":"4N6Me0GQECfZ"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nclasses = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\nplt.figure(figsize=(10, 20))\ncnt = 0\nfor i, cls_name in enumerate(classes):\n    path = f'{cls_name}/{cls_name}1.png'\n    # benign image\n    cnt += 1\n    plt.subplot(len(classes), 4, cnt)\n    im = Image.open(f'./data/{path}')\n    logit = model(transform(im).unsqueeze(0).to(device))[0]\n    predict = logit.argmax(-1).item()\n    prob = logit.softmax(-1)[predict].item()\n    plt.title(f'benign: {cls_name}1.png\\n{classes[predict]}: {prob:.2%}')\n    plt.axis('off')\n    plt.imshow(np.array(im))\n    # adversarial image\n    cnt += 1\n    plt.subplot(len(classes), 4, cnt)\n    im = Image.open(f'./fgsm/{path}')\n    logit = model(transform(im).unsqueeze(0).to(device))[0]\n    predict = logit.argmax(-1).item()\n    prob = logit.softmax(-1)[predict].item()\n    plt.title(f'adversarial: {cls_name}1.png\\n{classes[predict]}: {prob:.2%}')\n    plt.axis('off')\n    plt.imshow(np.array(im))\nplt.tight_layout()\nplt.show()","metadata":{"id":"RxNrXHKsEDGx","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"84300977-08c2-4617-8510-c66cd03eafed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Report Question\n* Make sure you follow below setup: the source model is \"resnet110_cifar10\", applying the vanilla fgsm attack on `dog2.png`. You can find the perturbed image in `fgsm/dog2.png`.","metadata":{"id":"WDc6QllJEHiC"}},{"cell_type":"code","source":"# original image\npath = f'dog/dog2.png'\nim = Image.open(f'./data/{path}')\nlogit = model(transform(im).unsqueeze(0).to(device))[0]\npredict = logit.argmax(-1).item()\nprob = logit.softmax(-1)[predict].item()\nplt.title(f'benign: dog2.png\\n{classes[predict]}: {prob:.2%}')\nplt.axis('off')\nplt.imshow(np.array(im))\nplt.tight_layout()\nplt.show()\n\n# adversarial image \nadv_im = Image.open(f'./fgsm/{path}')\nlogit = model(transform(adv_im).unsqueeze(0).to(device))[0]\npredict = logit.argmax(-1).item()\nprob = logit.softmax(-1)[predict].item()\nplt.title(f'adversarial: dog2.png\\n{classes[predict]}: {prob:.2%}')\nplt.axis('off')\nplt.imshow(np.array(adv_im))\nplt.tight_layout()\nplt.show()","metadata":{"id":"XhFVWA6JEH8Z","colab":{"base_uri":"https://localhost:8080/","height":957},"outputId":"99a7ff78-5166-4ddd-f474-3b599a19a31c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Passive Defense - JPEG compression\nJPEG compression by imgaug package, compression rate set to 70\n\nReference: https://imgaug.readthedocs.io/en/latest/source/api_augmenters_arithmetic.html#imgaug.augmenters.arithmetic.JpegCompression\n\nNote: If you haven't implemented the JPEG compression, this module will return an error. Don't worry about this.","metadata":{"id":"NfwhnywXEMwZ"}},{"cell_type":"code","source":"import imgaug.augmenters as iaa\n\n# pre-process image\nx = transforms.ToTensor()(adv_im)*255\nx = x.permute(1, 2, 0).numpy()\nx = x.astype(np.uint8)\n\n# TODO: use \"imgaug\" package to perform JPEG compression (compression rate = 70)\n# compressed_x =  ... x .. \n\nlogit = model(transform(compressed_x).unsqueeze(0).to(device))[0]\npredict = logit.argmax(-1).item()\nprob = logit.softmax(-1)[predict].item()\nplt.title(f'JPEG adversarial: dog2.png\\n{classes[predict]}: {prob:.2%}')\nplt.axis('off')\n\n\nplt.imshow(compressed_x)\nplt.tight_layout()\nplt.show()","metadata":{"id":"y2T7-L-BEKYg","colab":{"base_uri":"https://localhost:8080/","height":237},"outputId":"366430ea-0da5-4112-d1dc-b348ebb1b010","trusted":true},"execution_count":null,"outputs":[]}]}